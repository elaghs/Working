\section{Inductive Validity Cores}
\label{sec:support}

\newcommand{\bq}{\textsc{BaseQuery}\xspace}
\newcommand{\iq}{\textsc{InductiveQuery}\xspace}
\newcommand{\fq}{\textsc{FullQuery}\xspace}

\newcommand{\mink}{\textsc{MinimizeK}\xspace}
\newcommand{\reduceinv}{\textsc{ReduceInvariants}\xspace}
\newcommand{\minsupport}{\textsc{MinimizeSupport}\xspace}

\newcommand{\checksat}{\textsc{CheckSat}\xspace}
\newcommand{\unsatcore}{\textsc{UnsatCore}\xspace}
\newcommand{\unsat}{\textsc{UNSAT}\xspace}
\newcommand{\sat}{\textsc{SAT}\xspace}

Given a transition system which satisfies a safety property $P$, we
want to know which parts of the system are necessary for satisfying
the safety property. One possible way of asking this is, ``What is the
most general version of this transition system that still satisfies
the property?'' The answer is disappointing. The most general system is
$I(s) = P(s)$ and $T(s, s') = P(s')$, i.e., you start in any state
satisfying the property and can transition to any state that still
satisfies the property. This answer gives no insight into the original
system because it has no connection to the original system. In this
section we introduce the notion of {\em inductive validity cores} (IVC) 
which looks at generalizing the original transition system while 
preserving a safety property.

In order to talk about generalizing a transition system, we assume the
transition relation of the system has the structure of a top-level
conjunction. This assumption gives us a structure that we can easily
manipulate as we generalize the system. For ease of notation we will
write the transition system $T_1(s, s') \land \cdots \land T_n(s, s')$
as just $T_1 \land \cdots \land T_n$ or for short $\bigwedge T$ or %$\bigwedge_{k=1}^{n} T_{k}$ or
$\widehat T$ where $T = \{T_1, \ldots, T_n\}$.  We will use a similar
notation of sets of invariants. We now define our notion of
generalization for transition systems.

\mike{The $\bigwedge T$ and $\bigwedge S$ notation is a little bit ambigious, because we have already defined $T$ to be the conjunction of its sub-elements.  Is there a slight modification to make this clear?  Do we need it?  For Algorithm 1, could we just say $x$ is a conjunct in $S$? Then use $\widehat S \ {x}$ as notation? }
  
%\mike{I have some questions about the definitions below.  $\bigwedge T$ is a finite unrolling of a transition system, not a transition relation itself - this has a 


\begin{definition}{\emph{Inductive Validity Core:}}
  \label{def:ivc}
  Let $(I, \widehat T)$ be a transition system and let $P$ be a
  safety property with $(I, \widehat T)\vdash P$. We say $S \subseteq
  T$ is a {\em inductive validity core} for $(I, \widehat T)\vdash P$ iff $(I,
  \widehat S) \vdash P$. When $I$, $T$, and $P$ can be inferred from
  context we will simply say $S$ is an inductive validity core.
\end{definition}

\begin{definition}{\emph{Minimal Inductive Validity Core:}}
  \label{def:minimal-set-of-support}
  An inductive validity core $S$ for $(I, \widehat T)\vdash P$ is minimal iff
  there does not exist $M \subset S$ such that $M$ is an inductive validity core
  for $(I, \widehat T)\vdash P$.
\end{definition}

\begin{lemma}
  \label{lem:set-of-support-monotonic}
  Let $(I, \widehat T)$ be a transition system and let $P$ be a
  safety property with $(I, \widehat T)\vdash P$. Let $S_1 \subseteq
  S_2 \subseteq T$. If $S_1$ is an inductive validity core for $(I, \widehat
  T)\vdash P$ then $S_2$ is an inductive validity core for $(I, \widehat T)\vdash P$.
\end{lemma}
\begin{proof}
  From $S_1 \subseteq S_2$ we have $\widehat S_2 \Rightarrow \widehat
  S_1$. Thus the reachable states of $(I, \widehat S_2)$ are a subset
  of the reachable states of $(I, \widehat S_1)$. \qed
\end{proof}

\begin{algorithm}[t]
  \SetKwInOut{Input}{input}
  \SetKwInOut{Output}{output}
  \Input{$(I, \widehat T)\vdash P$}
  \Output{Minimal inductive validity core for $(I, \widehat T)\vdash P$}
  \BlankLine
  $S \leftarrow T$ \\
  \For{$x \in S$} {
    \If{$(I, \bigwedge (S\setminus\{x\})) \vdash P$}{
      $S \leftarrow S\setminus \{x\}$
    }
  }
  \Return{S}
\caption{Simple algorithm for computing a minimal inductive validity core}
\label{alg:naive}
\end{algorithm}

This lemma gives us a simple, but inefficient algorithm for computing
a minimal inductive validity core, Algorithm~\ref{alg:naive}. The resulting set
of this algorithm is obviously an inductive validity core for $(I, \widehat
T)\vdash P$. The following lemma shows that it is also minimal.

\begin{lemma}
  The result of Algorithm~\ref{alg:naive} is a minimal inductive validity core
  for $(I, \widehat T)\vdash P$.
\end{lemma}
\begin{proof}
  Let the result be $R$. Suppose towards contradiction that $R$ is not
  minimal. Then there an inductive validity core $M$ with $M \subset R$. Take $x
  \in R\setminus M$. Since $x \in R$ it must be that during the
  algorithm $(I, \bigwedge(S\setminus\{x\}))\vdash P$ is not true for
  some set $S$ where $R \subseteq S$. We have $M \subset R \subseteq
  S$ and $x\not\in M$, thus $M \subseteq S\setminus \{x\}$. Since $M$
  is an inductive validity core, Lemma~\ref{lem:set-of-support-monotonic} says
  that $S\setminus \{x\}$ is an inductive validity core, and so $(I, \bigwedge
  (S\setminus\{x\}))\vdash P$. This is a contradiction, thus $R$ must
  be minimal.
\end{proof}

This algorithm has two problems in practice. First, checking if a
safety property holds is undecidable in general thus the algorithm may
never terminate even when the safety problem is easily provable over
the original transition system. Second, this algorithm is very
inefficient since it tries to re-prove the property multiple times.

\begin{algorithm}[t]
  \SetKwInOut{Input}{input}
  \SetKwInOut{Output}{output}
  \Input{$P$ with invariants $Q$ is $k$-inductive for $(I,
    \widehat T)$}
  \Output{Inductive validity core for $(I, \widehat T)\vdash P$}
  \BlankLine
  $k \leftarrow \mink(I, \widehat T, P \land \widehat Q)$ \\
  $R \leftarrow \reduceinv_k(\widehat T, Q, P)$ \\
  \Return{$\minsupport_k(I, T, R)$}\\
\caption{Efficient algorithm for computing a nearly minimal inductive validity core}
\label{alg:set-of-support}
\end{algorithm}

The key to a more efficient algorithm is to make better use of the
information that comes out of model checking. In addition to knowing
that $P$ holds on a system $(I, \widehat T)$, suppose we also know
something stronger: $P$ with the invariant set $Q$ is $k$-inductive
for $(I, \widehat T)$. This gives us the broad structure of a proof
for $P$ which allows us to reconstruct the proof over a modified
transition system. However, we must be careful since this proof
structure may be more than is actually needed to establish $P$. In
particular, $Q$ may contain unneeded invariants which could cause the
inductive validity core for $P \land \widehat Q$ to be larger than the inductive validity core for $P$. Thus before computing the inductive validity core we first try
to reduce the set of invariants to be as small as possible. This
operation is expensive when $k$ is large so as a first step we
minimize $k$. This is the motivation behind
Algorithm~\ref{alg:set-of-support}.

\begin{figure}
\begin{align*}
  &\bq_1(I, T, P) \equiv \forall s_0.~ I(s_0) \Rightarrow P(s_0) \\
%%%
  &\bq_{k+1}(I, T, P) \equiv \bq_k(I, T, P) \land~ \\
%
  &\hspace{10pt}\left(\forall s_0, \ldots, s_k.~ I(s_0) \land T(s_0,
  s_1) \land \cdots \land T(s_{k-1}, s_k) \Rightarrow P(s_k)\right)
  \\[5pt]
%%%
  &\iq_k(T, Q, P) \equiv (\forall s_0, \ldots, s_k.~\\
%
  &\hspace{10pt} Q(s_0) \land T(s_0,
  s_1) \land \cdots \land Q(s_{k-1}) \land T(s_{k-1}, s_k) \Rightarrow
  P(s_k)) \\[5pt]
%%%
  &\fq_k(I, T, P) \equiv \\
%
  &\hspace{10pt}\bq_k(I, T, P) \land \iq_k(T, P, P)
\end{align*}
\caption{$k$-induction queries}
\label{fig:queries}
\end{figure}

To describe the details of Algorithm~\ref{alg:set-of-support} we
define queries for the base and inductive steps of $k$-induction
(Figure~\ref{fig:queries}). Note, in $\iq(T, Q, P)$ we separate the
assumptions made on each step, $Q$, from the property we try to show
on the last step, $P$. We use this separation when reducing the set of
invariants.

We assume that our queries are checked by an SMT solver. That is, we
assume we have a function \checksat which determines if an
existentially quantified formula is satisfiable or not. In order to
efficiently manipulate our queries, we assume the ability to create
{\em activation literals} which are simply distinguished Boolean
variables. These activation literals are automatically held true when
calling \checksat. In the case when the formula is unsatisfiable we
assume we have a function \unsatcore which returns a minimal set of
the activation literals such that the formula is unsatisfiable with
those activation literals held true. In practice, SMT solvers often
return a non-minimal set, but we can minimize the set via repeated
calls to \checksat.

\begin{algorithm}[t]
  $k' \leftarrow 1$ \\
  \While{$true$} {
    \If{$\checksat(\neg\iq_{k'}(T, P, P)) = \unsat$} {
      \Return{$k'$} \\
    }
    $k' \leftarrow k' + 1$ \\
  }
\caption{$\mink(T, P)$}
\label{alg:minimize-k}
\end{algorithm}

The function $\mink(T, P)$ is defined in
Algorithm~\ref{alg:minimize-k}. This function assumes that $P$ is
$k$-inductive for $(I, T)$. It returns the smallest $k'$ such that $P$
is $k'$-inductive for $(I, T)$. We start checking at $k' = 1$ since
smaller values of $k'$ are much quicker to check than larger ones. The
checking must eventually terminate since $P$ is $k$-inductive. We also
only check the inductive query since we know the base query will be
true for all $k' \leq k$. Although we describe each query in
Algorithm~\ref{alg:minimize-k} separately, in practice they can be done
incrementally to improve efficiency.

\begin{algorithm}[t]
  $R \leftarrow \{P\}$ \\
  Create activation literals $A = \{a_1, \ldots, a_n\}$ \\
  $C \leftarrow \{a_1 \Rightarrow Q_1, \ldots, a_n \Rightarrow Q_n\}$ \\
  \While{$true$} {
    $\checksat(A, \neg\iq_k(T, \widehat C, \widehat R))$ \\
    \If{$\unsatcore() = \emptyset$}{
      \Return{R}
    }
    \For{$a_i \in \unsatcore()$}{
      $R \leftarrow R \cup \{Q_i\}$ \\
      $C \leftarrow C \setminus \{a_i \Rightarrow Q_i\}$ \\
    }
  }
\caption{$\reduceinv_k(T, \{Q_1, \ldots, Q_n\}, P)$}
\label{alg:reduce-invariants}
\end{algorithm}

The function $\reduceinv_k(T, \{Q_1, \ldots, Q_n\}, P)$ is defined in
Algorithm~\ref{alg:reduce-invariants}. This function assumes that $P
\land Q_1 \land \cdots \land Q_n$ is $k$-inductive for $(I, T)$. It
returns a set $R \subseteq \{P, Q_1, \ldots, Q_n\}$ such that
$\widehat R$ is $k$-inductive for $(I, T)$. Like \mink, this function
only checks the inductive query since each element of $R$ is an
invariant and therefore will always pass the base query. A significant
complication for reducing invariants is that some invariants may
mutually need each other, even though none of them is needed to prove
$P$. Thus in Algorithm~\ref{alg:reduce-invariants} we find a minimal
set of invariants needed to prove $P$, then we find a minimal set of
invariants to prove those invariants, and so on. We terminate when no
more invariants are needed to prove the properties in $R$. 

This iterative lemma determination does not guarantee a minimal result. For example, we may
find $P$ requires just $Q_1$, that $Q_1$ requires just $Q_2$, and that
$Q_2$ does not require any other invariants. This gives the result
$\{P, Q_1, Q_2\}$, but it may be that $Q_2$ alone is enough to prove
$P$ thus the original result is not minimal. Also note, we do not care
about the result of \checksat, only the \unsatcore that comes out of
it. Since $P \land Q_1 \land \cdots \land Q_n$ is $k$-inductive, we
know the \checksat call will always return \unsat.

\mike{I think we want to be slightly more formal in our reasoning about these algorithms, defining soundness (and when possible, minimality) lemmas for each.}

\begin{algorithm}[t]
  Create activation literals $A = \{a_1, \ldots, a_n\}$ \\
  $T \leftarrow (a_1 \Rightarrow T_1) \land \cdots \land (a_n \Rightarrow T_n)$ \\
  $\checksat(A, \neg\fq_k(I, T, P))$ \\
  $R \leftarrow \emptyset$ \\
  \For{$a_i \in \unsatcore()$}{
    $R \leftarrow R \cup \{T_i\}$
  }
  \Return{R}
\caption{$\minsupport_k(I, \{T_1, \ldots, T_n\}, P)$}
\label{alg:minimize-support}
\end{algorithm}

The function $\minsupport_k(I, \{T_1, \ldots, T_n\}, P)$ is defined in
Algorithm~\ref{alg:minimize-support}. This function assumes that $P$
is $k$-inductive for $(I, \widehat T)$. It returns a minimal inductive validity core $R \subseteq \{T_1, \ldots, T_n\}$ such that $P$ is
$k$-inductive for $(I, \widehat R)$.

Our complete of support algorithm in
Algorithm~\ref{alg:set-of-support} does not guarantee a minimal inductive validity core. One reason is that \reduceinv does not guarantee a minimal
set of invariants. A larger reason is that we only consider the
invariants that the algorithm is given at the outset. It is possible
that there are other invariants which could lead to a smaller inductive validity core, but we do not search for them. In Section~\ref{sec:exprm}, we
show that in practice our algorithm is nearly minimal and much more
efficient that the naive algorithm.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main.tex"
%%% End

%%  LocalWords:  Lustre iff TODO invariants Minimality BaseQuery
%%  LocalWords:  InductiveQuery FullQuery MinimizeK ReduceInvariants
%%  LocalWords:  MinimizeSupport CheckSat UnsatCore UNSAT
