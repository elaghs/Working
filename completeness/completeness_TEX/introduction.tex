\section{Introduction}
\label{sec:intro}
%Do we have adequate requirements for our software?  This question has bedeviled the software development community perhaps as long as software has been written.  It has been a subject of active research since at least 1979, when it was the focus of the original TRW technical report on ``requirements engineering''~\cite{}.  A widely accepted view from Zowghi et. al~\cite{Zowghi} is that requirements adequacy can be defined in terms of the `three Cs'': Consistency, Completeness, and Correctness, where consistency is concerned with determining whether any requirements conflict with one another, completeness examines whether you have enough requirements, and correctness examines each requirement to determine whether it correctly characterizes a system behavior.  The focus of this paper is on {\em completeness}, which has been considered the most difficult adequacy criteria to define and analyze (Davis~\cite{}).

%\mike{TRW report: Software Requirements Engineering Methodology (Development) Alford, M. W. and Lawson,J. T. TRW Defense and Space Systems Group. 1979.}

%Boehm~\cite{}\ela{fill in} states that a complete set of requirements has three characteristics: (1) No information is left unstated or ``to be determined'', (2) The information does not contain any undefined objects or entities, (3) No information is missing from this document.  The first two characteristics are typically referred to as internal completeness.  {\em External} completeness (characteristic 3) of the document ensures that all of the information required for problem definition is found within the specification.  {\em Assessing} external completeness in a precise and formal way is difficult, because there is rarely an external reference that can be used to determine whether all relevant requirements have been defined.

%One approach for measuring completeness (\cite{Heimdahl_ConstencyAndCompleteness, Others}) examines whether the requirements form a {\em complete function} over inputs (or input traces, for embedded systems); that is, for a given input (trace), the requirements define a deterministic output (trace).  However, definition allows little freedom in implementations, and is usually considered too strict.  Instead, usually the {\em relative completeness} of requirements with respect to some other artifact.  The other artifact is often some implementation of the requirements.  This idea underlies certification standards such as DO178B/C~\cite{}, which require that requirements-based tests are sufficient to achieve structural coverage of the source code to a certain level of rigor.  If the tests are insufficient, then additional tests (and often, additional requirements) must be written.

%which could be source code, object code, an abstract ``model'' of the implementation, or even lower-level requirements.
%More recent work by Zeller~\cite{schuler_assessing_2011} and Murugesan~\cite{murugesan2015we} have attempted to adapt these measure towards automated test generation by examining coverage of {\em assertions} in the code.

%The testing approach requires that an implementation and test vectors to exist prior to performing this analysis, which has several drawbacks.  If the implementation is only available late in the development process, then incompleteness in requirements is not exposed until the implementation is constructed, potentially leading to substantial rework.  In addition, for large systems, large numbers of tests must be constructed, and these must be updated as requirements change.  Finally, the test metrics tend to substantially overapproximate which portions of a program are necessary to fulfill a requirement~\cite{Whalen13:OMCDC, chelenski1994oapplicability}.

For critical systems, it has been argued that formal methods
%, involving formalized requirements and proofs of implementation correctness,
should be applied to gain higher assurance than is possible with testing~\cite{Miller10:CACM,Rushby09:SEFM,Hardin09:Security}.  For these approaches, testing may still be performed, but the verification effort is primarily focused on performing proofs.  Unfortunately, proof-based approaches tend not to answer the question as to whether implementations have {\em additional functionality} that is not covered by requirements.  Testing, despite its faults, can measure {\em structural coverage} to find untested functionality and can find some errors by {\em serendipity}, in which problems not directly related to the requirement under test are exposed.  Therefore, in formal verification approaches, it is even more important that requirements be complete.

Relatively recently, techniques have been devised for analyzing completeness of requirements against formal implementation models, specified as transition systems or Kripke structures \cite{chockler2001practical,das2005formal, claessen2007coverage, grosse2007estimating}.  These models are agnostic to the abstraction level of the implementation: implementations can be lower-level requirements, software architectures, or concrete implementations of system behavior.  The mechanism used is based on {\em mutation} and {\em proof}: is it possible to prove that the requirements still hold of the system after mutating the model in some way?  If so, then the requirements are incomplete with respect to that the mutant part of the model.

Unfortunately, previous coverage metrics can {\em underapproximate} which portions of a program are necessary to fulfill the requirements.  That is, if we construct a model consisting of only the model elements marked as covered by the analysis, it is often no longer possible to prove the requirement.  Thus, the feedback provided to the developer may be somewhat misleading.
In addition, the mutation-based analyses tend to be very computationally expensive because there are many possible mutant models to verify.  For example, for model checkers, state of the art techniques have runtimes of (in the best case) several times more than is required for proof~\cite{chockler2010coverage}.

What we would like to have is an approach for checking the relative completeness of requirements against an implementation model that:
\begin{itemize}
    \item Can be applied early and throughout a development cycle on different implementation artifacts.
    \item Is accurate: the portion of the implementation that is identified as necessary demonstrates the
        fulfillment of the requirement but does not contain irrelevant information.
    \item Is reasonably computationally efficient.
\end{itemize}

\noindent Towards this end, we propose several notions of requirements completeness that examine {\em minimal proofs of requirements}.  In this approach, we measure the completeness of a set of requirements by examining an (approximately) minimal set of model elements necessary to construct a proof of all the requirements.  Like earlier approaches proposed for formal verification, this idea is implementation agnostic, so can be applied early in the development cycle against abstract implementation models.  The approach is implemented using {\em Inductive Validity Cores} (IVCs)~\cite{Ghass16} for transition systems.  We demonstrate that the IVC-based approach is considerably more computationally tractable than previous approaches based on mutation, averaging ~10\% overhead over model-checking alone, rather than (for our benchmark problems) ~900\% overhead required for the state of the art of the mutation-based metrics.  In addition, by definition, the covered portion of the model computed by the IVC-based approach is minimal (does not include unnecessary parts irrelevant to the property) and sufficient to prove the requirements.

Thus, the contributions of this work are:
\begin{enumerate}
\item A family of coverage metrics for formal verification based on \emph{minimal} Inductive Validity Cores (MIVCs) that preserve provability,
\item An implementation that efficiently measures property coverage over symbolic transition systems,
%%b.) more accurate than test-based methods,
\item An experiment that examines our technique against a state of the art mutation-based notion of completeness.
\end{enumerate}

\noindent Our eventual goal is to provide a range of metrics for quantifying requirements completeness using formal methods that is acceptable to certification authorities.  We believe that our minimal proof metrics provides reasonable candidates for this purpose.

%\mike{something here about certification?}

The rest of the paper is organized as follows.  In Section~\ref{sec:example}, we present a running example to illustrate the concepts formalized throughout the paper.  In Section~\ref{sec:background}, we provide the formal preliminaries for the approach and some background on mutation-based coverage notions.  Section~\ref{sec:method} presents our method for computing relative completeness.
In Section~\ref{sec:illust}, we illustrate how the new approach can be employed in the assessment of requirements completeness.
Section~\ref{sec:impl} provides detail on algorithms and implementation. Section~\ref{sec:experiments} describes an experiment to evaluate our algorithm comparing it with recent work by Chockler and Kroening~\cite{chockler2010coverage}.
Section~\ref{sec:results} provides the experimental results followed by a discussion on limitations of all ``relative completeness'' algorithms in Section \ref{sec:discussion}.  In Section~\ref{sec:related}, we cover related work.  Finally, Section~\ref{sec:conclusion} mentions some conclusions and future work.

