\section{Introduction}
\label{sec:intro}
%Do we have adequate requirements for our software?  This question has bedeviled the software development community perhaps as long as software has been written.  It has been a subject of active research since at least 1979, when it was the focus of the original TRW technical report on ``requirements engineering''~\cite{}.  A widely accepted view from Zowghi et. al~\cite{Zowghi} is that requirements adequacy can be defined in terms of the `three Cs'': Consistency, Completeness, and Correctness, where consistency is concerned with determining whether any requirements conflict with one another, completeness examines whether you have enough requirements, and correctness examines each requirement to determine whether it correctly characterizes a system behavior.  The focus of this paper is on {\em completeness}, which has been considered the most difficult adequacy criteria to define and analyze (Davis~\cite{}).

%\mike{TRW report: Software Requirements Engineering Methodology (Development) Alford, M. W. and Lawson,J. T. TRW Defense and Space Systems Group. 1979.}

%Boehm~\cite{}\ela{fill in} states that a complete set of requirements has three characteristics: (1) No information is left unstated or ``to be determined'', (2) The information does not contain any undefined objects or entities, (3) No information is missing from this document.  The first two characteristics are typically referred to as internal completeness.  {\em External} completeness (characteristic 3) of the document ensures that all of the information required for problem definition is found within the specification.  {\em Assessing} external completeness in a precise and formal way is difficult, because there is rarely an external reference that can be used to determine whether all relevant requirements have been defined.

%One approach for measuring completeness (\cite{Heimdahl_ConstencyAndCompleteness, Others}) examines whether the requirements form a {\em complete function} over inputs (or input traces, for embedded systems); that is, for a given input (trace), the requirements define a deterministic output (trace).  However, definition allows little freedom in implementations, and is usually considered too strict.  Instead, usually the {\em relative completeness} of requirements with respect to some other artifact.  The other artifact is often some implementation of the requirements.  This idea underlies certification standards such as DO178B/C~\cite{}, which require that requirements-based tests are sufficient to achieve structural coverage of the source code to a certain level of rigor.  If the tests are insufficient, then additional tests (and often, additional requirements) must be written.

%which could be source code, object code, an abstract ``model'' of the implementation, or even lower-level requirements.
%More recent work by Zeller~\cite{schuler_assessing_2011} and Murugesan~\cite{murugesan2015we} have attempted to adapt these measure towards automated test generation by examining coverage of {\em assertions} in the code.

%The testing approach requires that an implementation and test vectors to exist prior to performing this analysis, which has several drawbacks.  If the implementation is only available late in the development process, then incompleteness in requirements is not exposed until the implementation is constructed, potentially leading to substantial rework.  In addition, for large systems, large numbers of tests must be constructed, and these must be updated as requirements change.  Finally, the test metrics tend to substantially overapproximate which portions of a program are necessary to fulfill a requirement~\cite{Whalen13:OMCDC, chelenski1994oapplicability}.

For critical systems, it has been argued that formal methods
%, involving formalized requirements and proofs of implementation correctness, 
should be applied to gain higher assurance than is possible with testing~\cite{ModelCheckingTakesOff,Miller,Rushby}.  For these approaches, testing may still be performed, but the verification effort is primarily focused in performing proofs.  Unfortunately, proof-based approaches tend not to answer the question as to whether implementations have {\em additional functionality} that is not covered by requirements.  Testing, despite its faults, can measure {\em structural coverage} to find untested functionality and can find some errors by {\em serendipity}, in which problems not directly related to the requirement under test are exposed.  Therefore, in formal verification approaches, it is even more important that requirements be complete.

Relatively recently, techniques have been devised for analyzing completeness of requirements against formal implementation models, specified as transition systems or Kripke structures \cite{chockler2001practical,das2005formal, claessen2007coverage, grosse2007estimating}.  These models are agnostic to the ``abstraction level'' of the implementation: they can represent lower-level requirements, software architectures, or concrete implementations of system behavior.  The mechanism used is based on {\em mutation} and {\em proof}: is it possible to prove that the requirements still hold of the system after mutating the model in some way?  If so, then the requirements are incomplete with respect to that model element.

Unfortunately, previous completeness metrics can {\em underapproximate} which portions of a program are necessary to fulfill the requirements.  That is, if we construct a model from the required residual model elements returned by the analysis, it is oftentimes no longer sufficient to prove the requirement, so it provides incorrect feedback to the developer.  In addition, these approaches tend to be very computationally expensive.  For example, for model checkers, state of the art techniques have runtimes of (in the best case) several times more than is required for proof~\cite{}.

What we would like to have is an approach for checking the relative completeness of requirements against an implementation model that:
\begin{itemize}
    \item Can be applied early and throughout a development cycle on different implementation artifacts
    \item Is accurate: the portion of the implementation that is identified as necessary demonstrates the
        fulfillment of the requirement but does not contain additional information.
    \item Is reasonably computationally efficient.
\end{itemize}

\noindent Towards this end, we propose a notion of requirements completeness that examines {\em minimal proofs of requirements}.  In this approach, we measure the completeness of a set of requirements by examining an (approximately) minimal set of model elements necessary to construct a proof of all the requirements.  Like earlier proof-based approaches, this idea is implementation agnostic, so can be applied early in the development cycle against abstract implementation models.  We then define an implementation of this idea using {\em Inductive Validity Cores} (IVCs)~\cite{Ghass16} for transition systems.  We demonstrate that the IVC-based approach is considerably more computationally tractable than previous approaches based on mutation, averaging ~10\% overhead over model-checking alone, rather than (for our benchmark problems) ~900\% overhead required for mutation-based metrics.  In addition, by definition, it retains the portion of the model necessary to prove the requirements.

Thus, the contributions of this work are:
\begin{enumerate}
\item A notion of requirements completeness based on a proof involving a minimal number of model elements
\item A realization of this idea for symbolic transition systems using {\em inductive validity cores} that is a.) inexpensive to compute, given a model-checking proof, b.) more accurate than test-based methods, and c.) preserves the ``provability'' property from the residual model.
\item An implementation that computes this notion of completeness
\item An experiment that examines our notion of requirements completeness against a previous mutation-based notion of completeness.
\end{enumerate}

\noindent Our eventual goal is to provide a definition of completeness of requirements that can be established using formal verification-based approaches that is acceptable to certification authorities.  We believe that using minimal proofs provides a reasonable candidate metric for this discussion.

%\mike{something here about certification?}

The rest of the paper is organized as follows.  In Section~\ref{sec:example}, we present a running example to illustrate the concepts formalized throught the paper.  In Section~\ref{sec:background}, we provide the formal preliminaries for the approach.  Sections~\ref{sec:method} and \ref{sec:impl} present our approach to computing relative completeness and compare it with several other related approaches.  Section~\ref{sec:experiments} describes an experiment to examine our algorithm with recent work by Chockler and Kroening~\cite{chockler2010coverage}. Section~\ref{sec:results} provides the experimental results with respect to algorithm performance and properties of the residual models following by a discussion on limitations of all ``relative completeness'' algorithms in Section \ref{sec:discussion}.  In Section~\ref{sec:related}, we talk about related work.  Finally, Section~\ref{sec:conclusion} mentions some conclusions and future work.

