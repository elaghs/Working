\section{Introduction}
\label{sec:intro}

In order to build software, one usually starts with {\em requirements}, a set of statements about what the software is intended to do, which is refined either prior to, or in tandem with, the software being developed.  Requirements are necessary to software both for shaping the development of the software and for determining its adequacy when performing verification activities.  Therefore, determining the {\em adequacy} of requirements is of substantial importance to the eventual quality of the software.

Zowghi and Gervasi~\cite{} define adequacy of requirements in terms of the ``three Cs'': Consistency, Completeness, and Correctness.  \mike{[MOST OF THE REST OF THIS PARAGRAPH IS BLATANTLY STOLEN FROM GERVASI - REWRITE]} Davis states that completeness is the most difficult of the specification attributes to
define and incompleteness of specification is the most difficult violation to detect~\cite{}.
According to Boehm~\cite{}, to be considered complete, the requirements document must exhibit three fundamental characteristics: (1) No information is left unstated or ``to be determined'', (2) The information does not contain any undefined objects or entities, (3) No information is missing from this document. The first two properties imply a closure of the existing information and are typically referred to as internal completeness.  The third property, however, concerns the external completeness of the document
\cite{}. External completeness ensures that all of the information required for problem definition is found within the specification.  However, {\em assessing} external completeness in a precise and formal way is difficult, if not impossible, because there is rarely an external reference that can be used to determine whether all relevant requirements have been defined.

What tends to happen instead is that we measure the {\em relative completeness} of requirements with respect to some other artifact.  Usually, the other artifact is some form of implementation of the requirements (it could an abstract ``model'' of the implementation, source code, or object code).  This idea underlies certification standards such as DO178B/C~\cite{}, which require that requirements-based tests are sufficient to achieve structural coverage of the code to a certain level of rigor.  More recent work by Zeller~\cite{} and Murugesan~\cite{} have attempted to adapt these measure towards automated test generation by examining coverage of {\em assertions} in the code.  

A drawback of the approach is that an implementation must exist prior to performing this analysis; if the implementation is only available late in the development process, then incompleteness in requirements is not exposed until very late in the development cycle, potentially leading to substantial rework.  Next, the approach usually requires thousands to hundreds of thousands of tests, which are expensive to construct and can be expensive to modify in the face of changing or incomplete requirements.  Finally, the test metrics that are used for measurement tend to substantially overapproximate which portions of a program are necessary to fulfill a requirement~\cite{} \mike{cite MCDC and OMCDC work here}.

In addition, what happens if we want to use formal methods to prove system requirements?  Arguably, proofs should lead to higher assurance than tests, leading to more confidence in system performance.  However, the problem of requirements completeness becomes, if anything, more critical.  Relatively recently, 
%
%\cite{} \mike{cite formal verification work here}, we have attempted to use 
%These problem is exacerbated if one wishes to use a formal verification to assess  
%
techniques have been devised for analyzing completeness of requirements against formal implementation models, specified as transition systems or Kripke structures~\cite{}\mike{Chockler, Kupferman, Vardi, Kroening, etc.}.  These models are agnostic to the ``abstraction level'' of the implementation: they can represent lower-level requirements, software architectures, or concrete implementations of system behavior.  The mechanism used is based on {\em mutation} and {\em proof}: is it possible to prove that the requirements still hold of the system after mutating the model in some way?  If so, then the requirements are incomplete with respect to that model element.  Unfortunately, these metrics can {\em underapproximate} which portions of a program are necessary to fulfill a requirement: the residual model returned by the analysis for the requirement is, in the general case, insufficient to prove the requirement.  In addition, these approaches tend to be very computationally expensive, having a runtime of (in the best case) approximately 5x the time required for model checking.

What we would like to have is an approach for checking the relative completeness of requirements against an implementation model that:
\begin{itemize}
    \item Can be applied early and throughout a development cycle on different implementation artifacts
    \item Is accurate: the portion of the implementation that is identified as necessary demonstrates the 
        fulfillment of the requirement but does not contain additional information.
    \item Is reasonably computationally efficient. 
\end{itemize}     

\noindent Towards this end, we propose a notion of requirements completeness that examines {\em minimal proofs of requirements}.  In this approach, we measure the completeness of a set of requirements by examining an (approximately) minimal set of model elements necessary to construct a proof of all the requirements.  Like earlier proof-based approaches, this idea is implementation agnostic, so can be applied early in the development cycle against abstract implementation models.  We then define an implementation of this idea using {\em Inductive Validity Cores} (IVCs)~\cite{} \mike{Cite our FSE paper} for transition systems.  We demonstrate that the IVC-based approach is considerably more computationally tractable than previous approaches based on mutation, averaging ~15\% overhead over model-checking alone, rather than (for our benchmark problems) ~900\% overhead required for mutation-based metrics.  In addition, by definition, it retains the portion of the model necessary to prove the requirements.

Thus, the contributions of this work are:
\begin{enumerate}
\item A notion of requirements completeness based on a proof involving a minimal number of model elements
\item A realization of this idea for symbolic transition systems using {\em inductive validity cores} that is a.) cheap to compute, given a model-checking proof, b.) more accurate than test-based methods, and c.) preserves the ``provability'' property from the residual model.
\item An implementation that computes this notion of completeness
\item An experiment that examines our notion of requirements completeness against a previous mutation-based notion of completeness.
\end{enumerate}

\noindent Our eventual goal is to provide a definition of completeness of requirements that can be established using formal verification-based approaches that is acceptable to certification authorities.  We believe that using minimal proofs provides a reasonable candidate metric for this discussion.

%\mike{something here about certification?}

In the rest of the paper is organized as follows.  In Section~\ref{sec:example}, we present a motivating example.  In Section~\ref{sec:background}, we provide the formal preliminaries for the approach.  In Section~\ref{sec:method} we present our approach to computing relative completeness and compare it with several other related approaches.  In Section~\ref{sec:experiments} we define an experiment to examine our algorithm with recent work by Chockler and Kroening~\cite{chockler2010coverage}.  In Section~\ref{sec:results} we describe our results with respect to algorithm performance and properties of the residual models, and discuss limitations of all ``relative completeness'' algorithms.  In Section~\ref{sec:related} we describe related work.  Finally, Section~\ref{sec:conclusion} describes conclusions and future work.

 
...\mike{fill in!}.




\iffalse
Different notions of coverage have been well defined in software testing, however, in formal verification, it is very complex to define and compute this notion.
Usually, coverage techniques in the property-based verification try to measure the quality of the specification with regards to the completeness of a set of properties.
In fact, the goal is to point out unspecified behaviors, hence the idea behind most of the existing work is to address the question of `have we specified enough properties (requirements)?'
Since the coverage notions are usually  and over-approximation, achieving a high coverage does not guarantee there will be no missing behavior. However, when the coverage is low, techniques will definitely reveal some unspecified cases \cite{claessen2007coverage}.
\fi 