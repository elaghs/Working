\section{Experiments}
\label{sec:experiments}

We would like to evaluate both the {\em efficiency} and {\em
  effectiveness} of the proof-based metric (\ivccov)~comparing it with previous work (\nondetcov) using the three described algorithms: \ucalg, \ucbfalg, and \mustalg. To this end, we investigate the following research questions:
\begin{itemize}
    \item \textbf{RQ1:} How computationally expensive is it to perform coverage analysis using the \ucalg, \ucbfalg, and \mustalg\ algorithms?
    \item \textbf{RQ2:} What are the differences in coverage for different models produced by the \ucalg, \ucbfalg, and \mustalg algorithms?
    %\item \textbf{RQ3:} How often does the previously proposed \nondetcov\ metric (\mustalg algorithm) not preserve provability?
    %How much does the potential \emph{non-minimality} of \ucalg affect coverage scores?
\end{itemize}

\subsection{Experimental Setup}

The benchmark contains 475 Lustre models, 395 from~\cite{Hagen08:FMCAD} and 80 industrial models derived from \cite{hilt2013} and other sources.  Most of the benchmark models from~\cite{Hagen08:FMCAD} are small (10kB or less, with 6-40 equations) and include a range of hardware benchmarks and software problems involving counters that are difficult to solve inductively.
The 80 industrial models each contain over 600 equations and are each $\geq$80kB in size.\footnote{Tools and results are accessible from \cite{anoexpr}.}

For each test model, we computed \ucalg, \ucbfalg, and \mustalg  on an
Intel(R) i5-4690, 3.50GHz, 16 GB memory machine running Linux.

\input{results}


