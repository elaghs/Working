\section{Experiment and Results}
\label{sec:exprm}

We would like to investigate both the {\em efficiency} and {\em minimality} of the inductive validity core algorithm (Algorithm~\ref{alg:ivc}) as compared to the brute-force algorithm (Algorithm~\ref{alg:naive}).  Efficiency is computed in terms of wall-clock time: how much overhead does the IVC algorithm introduce?  Minimality is determined by the size of the IVC: cores with a smaller number of variables are preferred to cores with a larger number of variables.  Finally, we are interested in the {\em diversity} of solutions: how often is it possible to generate multiple minimal IVCs?

The use of JKind allows additional dimensions to our investigation: it supports two different inductive algorithms: k-induction and PDR, and a ``fastest'' mode, that runs both algorithms in parallel.  In addition, JKind supports multiple back-end SMT solvers including Z3~\cite{}, yices~\cite{}, MathSAT~\cite{}, and SMTInterpol~\cite{}.  We would like to determine whether the choice of inductive algorithm affects the size of the IVC, whether different solvers are more or less efficient at producing IVCs, and whether running different solvers/algorithms leads to {\em diversity} of IVC solutions.

Therefore, we investigate the following research questions:
\begin{itemize}
    \item \textbf{RQ1:} How expensive is it to compute IVC?
    \item \textbf{RQ2:} How close to minimal are the support sets computed by Algorithm~\ref{alg:set-of-support}?
    \item \textbf{RQ3:} Does one solver/algorithm outperform the others in terms of efficiency or minimality?
    \item \textbf{RQ4:} How much {\em diversity} exists in the solutions produced by different configurations?
    \item \textbf{RQ5:} Does model size affect minimality and diversity of solutions?
\end{itemize}

\subsection{Experimental Setup}
In this study, we started from a suite of 700 Lustre models from a benchmark suite developed in~\cite{Hagen08:FMCAD}.  We augmented this suite with YYY additional models from recent verification projects on avionics and medical devices (~\cite{QFCS15:backes,hilt2013}).  \mike{MORE HERE...stats on size, reasons for add'l models.}

Each benchmark model has a single property to analyze.  For our purposes, we are only interested in models with a {\em valid} property (though it is perhaps worth noting that there is no additional computation---and thus no overhead---using the JKind IVC options for {\em invalid} properties).  The benchmark suite contains 405\mike{+XXX} models with valid properties, which we use as our test subjects.  Size information for the models is shown in Figure~\mike{DEFINE FIGURE}.  

For each test model, we computed IVC in 12+1 configurations: the twelve configurations were the cross product of all solvers {Z3, yices, MathSAT, SMTInterpol} and inductive algorithms {k-induction, PDR, fastest}, and the remaining (+1) configuration was an instance of Algorithm~\ref{alg:naive} run on Z3, which has, overall, been the best solver for model-checking problems in JKind.  In addition, for each of the 12 configurations, we ran an instance of JKind without IVC to examine overhead.  The experiments were run on an Intel(R) i5-2430M, 2.40GHz, 4GB memory machine.  The data gathered for each configuration of each model included the time required to check the model without IVC, with IVC, and also the set of elements in the computed IVC.

\iffalse
\begin{itemize}
    \item an algorithm to compute a truly minimal set of support, i.e. \texttt{JSupport}.
    \item given a LUS model, a static crawler which automatically marks all equations of a node in the initial support set of a property.
    \item some trackers that measure the verification time with/ without support computation.
   % \item some minor changes in the XML writers.
\end{itemize}

\mike{My thoughts on this section: mostly, it needs more structure: more information on the properties of the models: size, provenance, etc., a broken out subsection on the description of the experimental setup, etc}

\mike{I think we want to split out the results in another top-level section}

Experiment:
\begin{itemize}
    \item (Overview) describe research questions and goals.
    \item Experimental setup: tell me about the models: how many, how big are they?  Then, tell me about the experiment: the tool configurations, the machine used for test.
    \item Data generation: Describe what you measured for each model analysis.
\end{itemize}
\fi

