\chapter{Experiments}
\label{ch:experiment}

\newcommand{\takeaway}[1]{
\vspace{12pt}
\noindent\fbox{\parbox{\textwidth}{#1}}
\vspace{6pt}
}
We would like to evaluate the different algorithms presented in Chapter \ref{ch:ivc}.
 The use of \texttt{JKind} allows additional dimensions to our investigation: as mentioned before, it supports two different inductive algorithms: $k$-induction and PDR, and a ``fastest'' mode, that runs both algorithms in parallel.  Also, \texttt{JKind} supports multiple back-end SMT solvers including \texttt{Z3}~\cite{DeMoura08:z3}, \texttt{Yices}~\cite{Dutertre06:yices}, \texttt{MathSAT}~\cite{Cimatti2013:MathSAT}, and \texttt{SMTInterpol}~\cite{Christ2012:SMTInterpol}.

This chapter is organized in two parts. First we evaluate IVCs and their relationship to model checking algorithms.  There are two dominant model checking algorithms in modern solvers: PDR and $k$-induction, and we would like to determine (1) whether the choice of inductive algorithm affects the size of the IVCs calculated by \ucalg, (2) whether different solvers are more or less efficient at producing approximate minimal IVCs (\ucalg), and (3) whether running different solvers/algorithms leads to a diversity of solutions obtained by \ucalg .

Second, using the \ucalg\ algorithm as a baseline, we evaluate the performance the IVC algorithms presented in Chapter 3 in Section \ref{sec:exp1}. Broadly speaking, we are interested in the following aspects: (4) The runtime efficiency of all IVC algorithms, (5) The efficacy (in terms of minimality) of the \ucalg\ algorithm, (6) factors impacting performance of computing all IVCs.  In terms of the sixth question, experimental data was used to construct the online algorithm; an accurate accounting of the relative cost of SAT and UNSAT problems is necessary to create an efficient algorithm for this problem.

Finally, we examine the relationship between inductive validity cores and bounded validity cores in Section \ref{sec:exp3}.  There are many cases where systems become to large or complex to find an inductive proof; in such cases it is still possible to perform a bounded proof (in terms of the number of steps).  Bounded validity cores are always underapproximations of some IVC, so can be used to witness traceablity and adequacy relationships between properties and implementation elements.  Several questions are of interest: (1) how similar are BVCs of different depths to IVCs?  (2) how quickly do they tend to converge to a specific size?, and (3) is the converged set the same as one of the MIVCs?  This initial experiment is designed to lay the groundwork for future explorations of the relationship between these two concepts.

The initial experiments allow us to choose an optimal configuration for \texttt{JKind} based on which we can run our major experiments. The major experiments are conducted to evaluate the efficiency and efficacy the IVC algorithms presented in Chapter \ref{ch:ivc}.
In summary, we are interested in the following aspects:

\begin{itemize}
  \item Evaluating the performance of \ucalg\ algorithm and \aivcalg ,
  \item Evaluating the efficacy (minimality) of the \ucalg\ algorithm outcome,
  \item Effective factors on the performance of finding all minimal IVCs,
  \item Evaluating the online approach for generating all minimal IVCs,
  \item Studying validity cores in bounded model checking.
\end{itemize}
\noindent For this purpose, we organize the major experiments in different categories; Section \ref{sec:exp1} is about the evaluation of our two key proposed algorithms: \ucalg\ and \aivcalg . Then in Section \ref{sec:exp2}, we examine our approach for calculating all minimal IVCs in the online manner as described in \ref{sec:onaivc}. Finally in Section \ref{sec:exp3}, we investigate a couple of interesting research questions related to bounded validity cores.

\section{On the Relationship between Model Checking Algorithms and IVCs}
\label{sec:exprinit}
In this section we study the effect of different model checking algorithms/solvers on the performance and accuracy of the \ucalg\ algorithm. We started from a suite of 700 Lustre models developed
as a benchmark suite by Hagen and Tinelli~\cite{Hagen08:FMCAD}. We augmented this suite
with 81 additional models from recent verification projects including
avionics and medical devices~\cite{QFCS15:backes,hilt2013}. Most of
the benchmark models from~\cite{Hagen08:FMCAD} are small (with 6-40 equations) and contain a range of hardware benchmarks and
software problems involving counters. The additional models are much
larger: with over 300 up to 10000 equations. We added the new
benchmarks to better check the scalability for the tools, especially
with respect to the brute force algorithm.
%
%\mike{MORE HERE...stats on size, reasons for add'l models.}
Each benchmark model has a single property to analyze.  For our purposes, we are only interested in models with a {\em valid} property (though it is perhaps worth noting that there is no additional computation---and thus no overhead---using the \texttt{JKind} IVC options for {\em invalid} properties).  In our benchmark set, 295 models yield counterexamples, and 10 additional models are neither provable nor yield counterexamples in our test configuration (see next paragraph for configuration information).  The benchmark suite therefore contains 476 models with valid properties, which we use as our initial test subjects.

For each test model, we computed \ucalg\ in 12+1 configurations: the
twelve configurations were the cross product of all solvers \{\texttt{Z3},
\texttt{Yices}, \texttt{MathSAT}, \texttt{SMTInterpol}\} and inductive algorithms
\{$k$-induction, PDR, fastest\}, and the remaining (+1) configuration
was an instance of \bfalg\ run on \texttt{Yices}, which is the default solver in
JKind.
 In addition, for each of the 12 configurations, we ran an
instance of \texttt{JKind} without IVC to examine overhead. The initial experiments
were run on an Intel(R) i5-2430M, 2.40GHz, 4GB memory machine, with a
1 hour timeout for each analysis on any model. The data gathered for
each configuration of each model included the time required to check
the model without IVC, with IVC, and also the set of elements in the
computed IVC.\footnote{The benchmarks, all raw experimental results,
  and computed data are available on \cite{expr}.}

Note that not all analysis problems were solvable with all algorithms: for all solvers, $k$-induction (without IVC) was unable to solve 172 of the examples.  When comparing minimality of different solving algorithms, we only considered cases where both algorithms provided a solution.

For this study, we are interested in the following research questions:
\begin{itemize}
  \item \textbf{RQ1:} Does the choice of SMT solver affect the performance of \ucalg ?
  \item \textbf{RQ2:} Does the choice of SMT solver or algorithm used to produce a proof
(k-induction or PDR) matter in terms of the minimality of the IVCs generated by \ucalg ?
  \item \textbf{RQ3:} Do different solvers and algorithms
lead to different minimal cores for \ucalg ?
\end{itemize}

\vspace{0.1in}
\subsubsection{RQ1}
First, we examine the performance overhead of the \ucalg\ algorithm over the time necessary to find a proof using inductive model checking.  To examine this question, we use the default {\em fastest} option of JKind which terminates when either the $k$-induction or PDR algorithm finds a proof.  To measure the performance overhead of the \ucalg\ algorithm, we execute it over the proof generated by the {\em fastest} option.

Since the \ucalg\ algorithm uses the UNSAT core facilities of the
underlying SMT solver, the performance is dependent on the efficiency
of this part of the solver. Looking at Tables~\ref{tab:runtime-ucalg-solvers}
and~\ref{tab:overhead-ucalg-solvers}, it is possible to examine both the
computation time for analysis using the four solvers under evaluation
and the overhead imposed by the \ucalg\ algorithm.
Figure~\ref{fig:perf-solvers} allows a visualization of the runtime for
the \ucalg\ algorithm running different solvers. The lines are sorted individually based on the running time of the \ucalg\ for each model. The data suggests that
\texttt{Yices} (the default solver in \texttt{JKind}) and \texttt{Z3} are the most performant
solvers both in terms of computation time and overhead.

\begin{figure*}
  \centering
  \includegraphics[width=\textwidth]{figs/performance_solvers.png}
    \vspace{-0.1in}
  \caption{\ucalg\ performance on different solvers}
  \label{fig:perf-solvers}
\end{figure*}

\begin{table}
  \caption{\ucalg\ runtime with different solvers}
  \centering
  \begin{tabular}{ |c||c|c|c|c| }
    \hline
     runtime (sec) & min & max & mean & stdev \\[0.5ex]
    \hline\hline
 %   JSupport & 2.381 & 165.157 & 21.533 & 23.533 \\[0.5ex]
    Z3   & 0.005 & 2.335 & 0.192 & 0.355 \\[0.5ex]
    Yices &   0.014  & 13.297   & 0.589 & 1.473 \\[0.5ex]
    SMTInterpol& 0.029 & 19.254 &  1.396 & 2.991 \\[0.5ex]
    MathSAT & 0.011 & 86.421 &  3.071 & 10.403 \\[0.5ex]
    \hline
  \end{tabular} \\
  \label{tab:runtime-ucalg-solvers}
\end{table}

\begin{table}
  \caption{Overhead of \ucalg\ computations using different solvers}
  \centering
  \begin{tabular}{ |c||c|c|c|c| }
    \hline
     solver & min & max & mean & stdev \\[0.5ex]
    \hline
    Z3   & 0.73\% & 84.13\% & 17.38\% & 16.92\% \\[0.5ex]
    Yices &   0.17\%  & 351.47\%   & 52.20\% & 54.50\% \\[0.5ex]
   SMTInterpol& 1.46\% & 175.75\% &  46.81\% & 37.35\%\\[0.5ex]
    MathSAT & 0.78\% & 955.52\% &  80.21\% & 112.92\%\\[0.5ex]
    \hline
  \end{tabular}
  \label{tab:overhead-ucalg-solvers}
\end{table}

\vspace{0.1in}
\subsubsection{RQ2}
As described in Chapter~\ref{ch:ivc}, the \ucalg\
algorithm is not guaranteed to produce minimal cores due in part to
the role of invariants used in producing a proof; as $k$-induction and
PDR use substantially different invariant generation algorithms, it is
likely that the set of necessary invariants for proofs are dissimilar,
and that this would in turn affect the number of model elements required for
the proof.  It is possible that one or the other algorithm is more likely
to yield smaller invariant sets.  In addition, differences in the choice of the
UNSAT core algorithms in the different solvers could affect the size of the
generated core. However, our algorithm already performs a minimization
step on UNSAT cores, and thus the only differences would be due to one
algorithm leading to a different minimal core than another.

As mentioned, $k$-induction is unable to solve all of the analysis problems; therefore we include only models that are solvable using {\em both} $k$-induction and PDR by {\em all solvers}, 304 models in all.  Examining the aggregate data in Table~\ref{tab:minimality-algorithm-solvers}, we can see the sizes of cores produced by different algorithms and solvers.
\vspace{0.1in}
\subsubsection{RQ3}
In this section, we examine the issue of diversity:
do different solvers and algorithms lead to {\em different} minimal
cores? This is both a function of the models and the solution
algorithms: for certain models, there is only one possible minimal IVC
set, whereas other models might have many. Given that there are
multiple solutions, the interesting question is whether using
different solvers and algorithms will lead to different solutions.
The reason diversity is considered is that it has substantial relevance to
some of the uses of the tool, e.g., for constructing multiple traceability
matrices from proofs (see Section~\ref{sec:traceability}).
Note that our exploration in this experiment is not
exhaustive, but only exploratory, based on the IVCs returned by different
algorithms and tools; we leave exhaustive exploration of
IVCs for future work.

%Given diversity of results, we may wish to
%distinguish {\em must} traceability elements from {\em may}
%traceability elements across a set of diverse solutions, and consider
%more systematic explorations of diversity in future work.

To measure diversity of the generated IVCs, we use Jaccard distance:
\begin{definition}{\emph{Jaccard distance:}}
  \label{def:dj}
  $d_J(\small{A}, \small{B}) = 1 - \frac{|A \cap B|}{|A \cup B|} ,\\ 0 \leq d_J(\small{A}, \small{B}) \leq 1$
\end{definition}
\noindent Jaccard distance is a standard metric for comparing finite
sets (assuming that both sets are non-empty) by comparing the size of
the intersection of two sets over its union. For each model in the
benchmark, the experiments generated 13 potentially different IVCs. Therefore, we
obtained $\binom{13}{2} = 78$ combinations of pairwise distances per
model. Then, minimum, maximum, average, and standard deviation of the
distances were calculated (Figure~\ref{fig:jacdis}), by which, again,
we calculated these four measures among all models. As seen in
Table~\ref{tab:jaccard-avg}, on average, the Jaccard distance between
different solutions is small, but the maximum is close to 1, which
indicates that even for our exploratory analysis, there are models for
which the tools yield substantially diverse solutions. The diversity
between solutions is represented graphically in
Figure~\ref{fig:jacdis}, where for each model, we present the min,
max, and mean pairwise Jaccard distance of the solutions produced by algorithm
\ucalg\ for each model, sorted by the mean distance.

\begin{table}
  \caption{Pairwise Jaccard distances among all models}
  \centering
  \begin{tabular}{ |c|c|c|c| }
    \hline
     min & max & mean & stdev \\[0.5ex]
    \hline
    %sample size = 4196
     0.0   & 0.878 & 0.026 & 0.059 \\[0.5ex]
    \hline
  \end{tabular}
  \label{tab:jaccard-avg}
\end{table}

\begin{figure*}
  \centering
  \vspace{3mm}
  \includegraphics[width=\textwidth]{figs/jacdis2.png} \\
  \vspace{-0.1in}
  \caption{Pairwise Jaccard distance between IVCs}\label{fig:jacdis}
\end{figure*}

\iffalse

To measure the overall similarity among all sets, instead of a pairwise comparison, we used \emph{frequent pattern mining} \cite{han2007frequent}. To define an overall similarity among all sets of support of a given model\footnote{Note that all models in the benchmarks are single property; hence, instead of saying a set of support of a given \emph{property}, we just refer it as the support set of the \emph{model} while explaining the experimental results.}, we calculated a \emph{core} support set for each model in the benchmark, which can be considered as a closed frequent pattern; a core set of model $M$, denoted by $C_M$, is defined as:
\begin{definition}
  \label{def:core}
  $C_M = \bigcap_{i=1}^{13} s_{Mi},   \hspace{9pt} s_{Mi} \in S_M$
\end{definition}

Based on this notion, overall dissimilarity, denoted by $D_{J\{M\}}$, is defined as follows:

\begin{definition}
  \label{def:dis}
  $D_{J\{M\}} =  \frac{\sum_{i=1}^{12}d_J(s_{Mi}, C_M)}{12},   \hspace{9pt} s_{Mi} \in S_M$
\end{definition}

Since our goal is to measure the diversity or dissimilarity among sets computed by \texttt{ReduceSupport}, in \ref{def:dis}, we exclude the set generated by \ucbfalg . In Fig~\ref{fig:jacdis}, the \emph{overall distance} line shows $D_{J\{M\}}$ per model, which can be analyzed from the following hypotheses:
\begin{itemize}
  \item H0: variety of obtained sets of support is high (average $D_{J\{M\}}$ of 0.2)
  \item H1: variety of obtained sets of support is small (average $D_{J\{M\}}$ less than 0.2)
\end{itemize}
Table~\ref{tab:variety} shows that, with an effect size of 0.79, H0 can be rejected.
\begin{table}
  \caption{$D_{J\{M\}}$ among all models}
  \centering
  \begin{tabular}{ |c|c|c|c|c|c| }
    \hline
     min & max & mean & stdev & ES & p-value\\[0.5ex]
    \hline
    %sample size = 395
     0.0   & 0.879 & 0.099 & 0.141 & 0.72 & < 0.00001 \\[0.5ex]
    \hline
  \end{tabular}
  \label{tab:variety}
\end{table}
\fi
%summarized as follows:
%\begin{itemize}
%  \item minimum $D_{J\{M\}}$ among all models: 0.0
%  \item maximum $D_{J\{M\}}$ among all models: 0.879
%  \item average $D_{J\{M\}}$ among all models: 0.096
%  \item standard deviation of $D_{J\{M\}}$ among all models: 0.132
%\end{itemize}
%if we sum all elements of all cores together, that PDR has an smaller core size in aggregate than $k$-induction.
%However, the data is noisy, and to examine \textbf{RQ2.1} systematically, we construct a hypothesis that PDR will, in general, equal or outperform $k$-induction on an arbitrary model:

%\mike{ADD HYPOTHESIS/NULL HYPOTHESIS HERE}

%Although the aggregate data suggests that PDR will yield a smaller core (on average) than $k$-induction, this claim is not supported for a given model with significance.

%we already perform a linear scan of the cores generated by the SMT solver to remove unnecessary conjuncts

\begin{table}
  \caption{Aggregate IVC sizes produced by \ucalg\ using different inductive algorithms and solvers}
  \centering
  \begin{tabular}{ |c|c|c|c| }
    \hline
     solver & PDR & $k$-induction & \textbf{total} \\
    \hline
      Z3 & 2378 & 2379 & 4757 \\
      Yices & 2384 & 2376 & 4760 \\
      MathSAT & 2375 & 2369 & 4744 \\
      SMTInterpol & 2378 & 2368 & 4746 \\
    \hline
      \textbf{total} & 9515 & 9492 &   \\
    \hline
  \end{tabular}
  \label{tab:minimality-algorithm-solvers}
\end{table}



\section{On the efficiency and efficacy of different ``offline'' IVC algorithms}
\label{sec:exp1}
\input{expr1}

\section{On the relative performance of offline and online algorithms for all IVCs.}
\label{sec:exp2}
\input{expr}


\section{On the relationship of BVCs and MIVCs}
\label{sec:exp3}
\input{bvcexp}
