\section{Evaluation}
\label{sec:eval}
This section evaluates our approach by addressing the following research questions:

\ela{could you please rephrase these questions?! having hard time saying them clearly :-( }
\begin{itemize}
    \item \textbf{RQ1:} How much overhead does computation of set of support add to the proofs? 
    \item \textbf{RQ2:} How dependent is our approach on different tools and proof engines? Does different solvers/ proof engines generate different support set? If so, to what extent are they different?
    \item \textbf{RQ3:} Is there any relationship between the size of a computed support set and 
    solvers/ proof engines? 
    \item \textbf{RQ4:} Is there any relationship between the size of the model and the variety of support sets?
    \item \textbf{RQ5:} How close to minimal are the support sets computed by our algorithm? For each model, we have 12 sets of support computed by \texttt{JKind}, and one \emph{minimal} set computed by \texttt{JSupport}; we would like to know which configurations generated sets that are very close or far to the minimal one.
    \item \textbf{RQ6:} How do different solvers perform on computing support? And, how efficient is it in comparison with \texttt{JSupport}?
\end{itemize} 

\subsection{Analytical Results}
\label{sec:res}
This section answers the aforementioned research questions briefly describing the way the results are analyzed. Note that the results of 10 models with unprovable properties are omitted in calculations. In addition, while analyzing support sets, \texttt{K-induction} settings that timed out were not considered because they failed to prove the properties.

% RQ1: the overhead of support computation on different solvers
\textbf{RQ1.} In general, for timing analyses, from all 13 configurations, we only considered the settings where both \texttt{PDR} and \texttt{K-induction} engines were activated. Since this configuration is a default setting in \texttt{JKind}, we only care about timing while both engines are employed. Therefore, $(4 \times 405) = 1620$ of all 5265 runs have been analyzed to address time-efficiency questions. The overhead is defined as the percentage of the overall runtime is dedicated to support computation:
\mbox{$overhead\_percentage = 100 \times (support\_runtime \div overall\_runtime)$}.
 Table~\ref{tab:overhead} shows the overhead of support computation on different solvers.

\begin{table}
  \centering
  \begin{tabular}{ |c||c|c|c|c| }
    \hline
     solver & min & max & mean & stdev \\[0.5ex]
    \hline
    Z3   & 0.726\% & 45.396\% & 13.414\% & 11.369\% \\[0.5ex]
    Yices &   0.200\%  & 262.254\%   & 47.264\% & 51.193\% \\[0.5ex]
    SMTInterpol& 0.930\% & 268.571\% &  70.500\% & 58.541\%\\[0.5ex]
    MathSAT & 0.502\% & 396.124\% &  71.007\% & 79.990\%\\[0.5ex]
    \hline
  \end{tabular}
  \caption{\small{Overhead of support computation on different solvers}}
  \label{tab:overhead}
\end{table}

\noindent\fbox{%
    \parbox{\textwidth}{%
        In average, computation of support set has less than 50\% overhead. Averagely, if it takes \textit{t} to prove $\mathbb{P}$, it will take \textit{1.5t} to both prove $\mathbb{P}$ and compute its set of support.
    }%
}
 \vspace{9pt}

% RQ2: How dependent is our approach on different tools and proof engines?
\textbf{RQ2.} To answer this question, we analyze the results from different aspects. The following describes methods used for analyzing and their results.

\textbf{(1)} For each model in the benchmarks, the experiments generated 13 different sets of support. We used Jaccard distance to measure dissimilarity between pairwise of these sets:

\begin{center}
$d_J(\small{A}, \small{B}) = 1 - \frac{|A \cap B|}{|A \cup B|} ,\hspace{9pt} 0 \leq d_J(\small{A}, \small{B}) \leq 1$
\end{center}
\vspace{6pt} 

Therefore, we obtained $\binom{13}{2} = 78$ combinations of distances per model. Then, minimum, maximum, average, and standard deviation of the distances were calculated, by which, again, we calculate these four measures among all 405 models. Table~\ref{tab:pairwise-jd} represents the results of this analysis.

\begin{table}
  \centering
  \begin{tabular}{ |c|c|}
    \hline
     min $d_J$ among all models& 0.0 \\[0.5ex]
     \hline
     max $d_J$ among all models& 0.882\\[0.5ex]
     \hline
     mean $d_J$ among all models& 0.027\\[0.5ex]
     \hline
     stdev $d_J$ among all models& 0.062\\[0.5ex]
    \hline
  \end{tabular}
  \caption{\small{Pairwise Jaccard distance between support sets obtained from different configurations}}
  \label{tab:pairwise-jd}
\end{table}

\noindent\fbox{%
    \parbox{\textwidth}{%
        Support sets computed with different solvers and engines have an average Jaccard distance of 0.027, which implies our algorithm has very small dependency on tools and proof algorithms.
    }%
}
 \vspace{9pt}
 
\textbf{(2)} Since one goal was to know if sets of support are different, to what extent they are and which configurations have generated more different/similar sets. We analyzed the results, 
and it turns out 174 models out of 405 contain at least two different support sets (which means in 231 models, all 13 sets of support are the same). We analyzed those 174 models; for each of them pairwise Jaccard distance between sets compared, and configurations with maximum/ minimum distances are collected. Fig~\ref{fig:maxdis} and Fig~\ref{fig:mindis} show the results. For example, in Fig~\ref{fig:maxdis}, in 20 models out of 405, \texttt{JSupport} and the configuration where \texttt{Z3} and both proof engines were employed, Jaccard distance between support sets computed by them is maximum. As you can see, different configurations of \texttt{JKind} do not affect very much the variety of the generated sets. However, \texttt{JSupport} and \texttt{JKind} configurations have had maximum distances most of the time. But, even so, the frequency in 405 models is very low.


\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{figs/max_settings_analyses.png}
  \caption{Pairwise configurations with maximum Jaccard distance}\label{fig:maxdis}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{figs/min_settings_analyses.png}
  \caption{Pairwise configurations with minimum Jaccard distance}\label{fig:mindis}
\end{figure}


\noindent\fbox{%
    \parbox{\textwidth}{%
        Different solvers and proof engines have very small impact on the variety of elements in a support set of a property computed by our algorithm.
    }%
}
 \vspace{9pt}

\textbf{(3)} In addition to Jaccard distance, we also measured the similarity among all sets computed in different configurations per model. Let $S_M$ be a set of all support sets computed for model $M$ (i.e. in our experiments, $S_M$ is a set of 13 sets).  We define similarity per model as follows:

\begin{center}
$similarity = \frac{|\bigcap_{i=1}^{13} s_{Mi}|}{|\bigcup_{i=1}^{13} s_{Mi}|}, \hspace{9pt} s_{Mi} \in S_M$
\end{center}
\vspace{6pt} \ela{should see if this formula has any name in mathematics?!}

Needless to say, $0 \leq similarity \leq 1$, and if all the sets in $S_M$ are the same, similarity will be 1. So the more closer to 1 it is, the more similar sets we have. Table~\ref{tab:sim} is a summary of minimum, maximum, average, and standard deviation of similarity among all models. Fig~\ref{fig:sim} also shows similarity in all models.

\begin{table}
  \centering
  \begin{tabular}{ |c|c|}
    \hline
     min of similarity& 0.12 \\[0.5ex]
     \hline
     max of similarity& 1.0\\[0.5ex]
     \hline
     mean of similarity & 0.884\\[0.5ex]
     \hline
     stdev of similarity & 0.165\\[0.5ex]
    \hline
  \end{tabular}
  \caption{\small{Similarity among all models}}
  \label{tab:sim}
\end{table}

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{figs/similarity.png}
  \caption{Similarity measurement for all models}\label{fig:sim}
\end{figure}


\noindent\fbox{%
    \parbox{\textwidth}{%
        An average \textit{similarity} of 0.884 shows that support sets computed in different 
        configurations. So, the dependency of our algorithm to different solvers and proof engines is negligible.
    }%
}
 \vspace{9pt}

\textbf{(4)} We also calculate a core set of support for each model of the benchmarks; A core set of model $M$ is defined as
$\bigcap_{i=1}^{13} s_{Mi},   hspace{9pt} s_{Mi} \in S_M$. Then, the size difference of the core set with the smallest support set of $M$ was calculated. The results are visualized in Fig~\ref{fig:core}

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{figs/core.png}
  \caption{Size difference between the core set and the smallest support set per model}\label{fig:core}
\end{figure}


\noindent\fbox{%
    \parbox{\textwidth}{%
        The core support set is very close to the smallest support set of a property.
    }%
}
 \vspace{9pt}

% RQ3: Is there any relationship between the size of a computed support set and
%solvers/ proof engines? 
\textbf{RQ3:} To answer this question, we analyzed the raw data with two different approaches described in the following.
 
\textbf{(1)} We analyzed the sets of each 405 models. In each model, we looked which configuration generated the biggest support set and which one did the smallest. Fig~\ref{fig:smallset} and Fig~\ref{fig:bigset} visualize the result of this analysis. For example, as you can see in Fig~\ref{fig:bigset}, \texttt{JSupport} generated biggest support sets in less than 10 models. It should be mentioned that it means that there is at least one \texttt{JKind} configuration that generated the set with a smaller size.

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{figs/small_conf.png}
  \caption{Smallest support set (in terms of size) vs configurations}\label{fig:smallest}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{figs/big_conf.png}
  \caption{Biggest support set (in terms of size) vs configurations}\label{fig:bigest}
\end{figure}


\noindent\fbox{%
    \parbox{\textwidth}{%
    Solvers/ proof engines do not affect very much the size of the support set computed by \texttt{JKind}.
    }%
}
 \vspace{9pt}

\textbf{(2)} Since \texttt{JSupport}, with a great percentage, most of the time computed the smallest support set, we compared the size of the sets computed in each \texttt{JKind} configuration with \texttt{JSupport} per model. Fig~\ref{fig:minpdr} to Fig~\ref{fig:minboth} show the results. If we take a closer look at the results in the pictures, we can summarize them as in Table~\ref{tab:minimality}. For each configuration, we collected the difference between its support size and \texttt{JSupport} per model. Then, minimum, maximum, mean, and standard deviation of the collected data have been reported.

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{figs/minimality_pdr.png}
  \caption{Minimality comparison of \texttt{PDR}}\label{fig:minpdr}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{figs/minimality_kind.png}
  \caption{Minimality comparison of \texttt{K-induction}}\label{fig:minkind}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{figs/minimality_both.png}
  \caption{Minimality comparison of \texttt{K-induction} and \texttt{PDR} working together}\label{fig:minboth}
\end{figure}

\begin{table}
  \centering
  \begin{tabular}{|c|c|c|c|c|}
     \hline
     configuration & min & max & mean & stdev \\[0.5ex]
     \hline\hline
     K-induction & -2 & 28 & 0.542 & 1.831 \\[0.5ex]
     PDR & -2 & 28 & 0.677 & 1.819 \\[0.5ex]
     Both engines & -2 & 28 & 0.661 & 1.751 \\[0.5ex]
     \hline
     Z3, K-ind & -2 & 28 & 0.555 & 1.842 \\[0.5ex]
     Yices, K-ind & -2 & 28 & 0.544 & 1.838 \\[0.5ex]
     SMTInterpol, K-ind & -2 & 28 & 0.534 & 1.822 \\[0.5ex]
     MathSAT, K-ind & -2 & 28 & 0.537 & 1.823 \\[0.5ex]
     \hline
     Z3, PDR & -2 & 28 & 0.697 & 1.852 \\[0.5ex]
     Yices, PDR & -2 & 28 & 0.668 & 1.807 \\[0.5ex]
     SMTInterpol, PDR & -2 & 28 & 0.673 & 1.813 \\[0.5ex]
     MathSAT, PDR & -2 & 28 & 0.668 & 1.803 \\[0.5ex]
     \hline
     Z3, Both engines & -2 & 28 & 0.663 & 1.729 \\[0.5ex]
     Yices, Both engines & -2 & 28 & 0.650 & 1.721 \\[0.5ex]
     SMTInterpol, Both engines & -2 & 28 & 0.637 & 1.713 \\[0.5ex]
     MathSAT, Both engines & -2 & 28 & 0.692 & 1.837 \\[0.5ex]
     \hline     
   \end{tabular}
  \caption{\small{Summary of minimality analyses}}\label{tab:minimality}
\end{table}


\noindent\fbox{%
    \parbox{\textwidth}{%
     \texttt{JKind} is able to find minimal support sets with a very negligible dependency on solvers/ proof engines.
    }%
}
 \vspace{9pt}

% RQ4: Is there any relationship between the model size and variety of its support sets?
\textbf{RQ4.} The size of the models in our benchmark is between 0 KB and 10 KB. We divided the models in 9 different categories based on their sizes such that $category_i$ contains 
all models whose sizes are between $(i - 1)$ KB and $i$ KB. Then, using the \textit{similarity} formula defined in RQ2, the similarity for each model in each category was calculated. After that, minimum, maximum, mean, standard deviation of the data were obtained per category. Table~\ref{tab:modelsize} summarizes the result of this analysis; the column \emph{number} in the table shows the number of models in each category (for example, there are 6 models whose sizes are between 8 KB and 9 KB, and in all of them \textit{similarity} is 1.0).
 
 
\begin{table}
  \centering
  \begin{tabular}{ |c||c|c|c|c|c|}
    \hline
    size (KB) & number&
     min & max & mean & stdev \\
    \hline\hline
    [0-1] & 49 & 0.33 & 1.0 & 0.877 & 0.213 \\[0.5ex]
    [1-2] & 90& 0.3 & 1.0 & 0.835 & 0.192 \\[0.5ex]
    [2-3] & 26&0.5 & 1.0 & 0.876 & 0.131 \\[0.5ex]
    [3-4] & 34&0.57 & 1.0 & 0.896 & 0.158 \\[0.5ex]
    [4-5] & 88&0.12 & 1.0 & 0.895 & 0.153 \\[0.5ex]
    [5-6] & 11&0.75 & 1.0 & 0.83 & 0.107 \\[0.5ex]
    [6-7] & 2&0.96 & 1.0 & 0.98 & 0.02 \\[0.5ex]
    [7-8] & 99&0.28 & 1.0 & 0.916 & 0.124 \\[0.5ex]
    [8-9] & 6&1.0 & 1.0 & 1.0 & 0.0 \\[0.5ex]
    \hline
  \end{tabular}
  \caption{Model size vs similarity among its support sets}\label{tab:modelsize}
\end{table}


\noindent\fbox{%
    \parbox{\textwidth}{%
     The size of the model does not affect on the stability of our algorithm; 
     if the size of a model grows, it does not mean that model will have a lot of different minimal support sets. In other words, minimal support sets of a given property computed by \texttt{JKind} will be very similar to each other.
    }%
}
 \vspace{9pt}
 
% RQ5: 
\textbf{RQ5.} 

\textbf{(2)} The next approach we took to answer this question was to analyze the size of the biggest and smallest support sets versus \texttt{JSupport} per model. The results are shown in Fig~\ref{fig:minjsup}.
We computed the size of biggest and smallest set per model, then added them together for all models. The same calculation has been done for JSupport. The aggregate number of elements in the \emph{smallest} support sets is 3263. And, the number in the \emph{biggest} support sets is 3609. Finally, the aggregate number of elements in support sets computed by \texttt{JSupport} is 3078.

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{figs/minimality_analyses.png}
  \caption{Minimality comparison}\label{fig:minjsup}
\end{figure}


\noindent\fbox{%
    \parbox{\textwidth}{%

    }%
}
 \vspace{9pt}
  
 
% RQ6: overall runtime in JSupport and different solvers
\textbf{RQ6.} In addition to overhead, it is also important to know how efficiently \texttt{JKind} performs on computing set of support in comparison with \texttt{JSupport}. Table~\ref{tab:eff-comp-jsup} compares overall runtime of support computation in different solvers and \texttt{JSupport}.

\begin{table}
  \centering
  \begin{tabular}{ |c||c|c|c|c| }
    \hline
     runtime (sec) & min & max & mean & stdev \\[0.5ex]
    \hline\hline
    JSupport & 2.381 & 165.157 & 21.533 & 23.533 \\[0.5ex]
    Z3   & 0.112 & 42.928 & 2.412 & 5.009 \\[0.5ex]
    Yices &   0.111  & 39.657   & 2.464 & 5.224 \\[0.5ex]
    SMTInterpol& 0.225 & 514.886 &  4.331 & 26.411 \\[0.5ex]
    MathSAT & 0.111 & 43.623 &  2.765 & 5.157 \\[0.5ex]
    \hline
  \end{tabular}
  \caption{\small{\texttt{JKind} runtime with \emph{-support} option in different solvers compared with \texttt{JSupport}}}
  \label{tab:eff-comp-jsup}
\end{table}

%For calculations, we considered all settings where both \texttt{K-induction} and \texttt{PDR} were activated
% then in for all 405 models in everything, we collected runtime info, then calculated min/max/avg/stdev between them
% in other words, there were 4 settings to be considered: z3_both, yices_both, mathsat_both, smtinterpol_both
For 405 models, runtime of support computation in the configurations where both \texttt{K-induction} and \texttt{PDR} were activated has been collected. Fig~\ref{fig:runtimez3} and Fig~\ref{fig:runtimeall} visualize the results.

%\begin{figure}
%\centering
%\begin{tabular}[c]{cc}
%    \begin{subfigure}[b]{0.20\textwidth}
%      \includegraphics[width=\textwidth]{figs/figure_1.png}
%    \end{subfigure}&
%    \begin{subfigure}[b]{0.20\textwidth}
%      \includegraphics[width=\textwidth]{figs/figure_z3_zoom.png}
%    \end{subfigure}\\
%    \begin{subfigure}[b]{0.20\textwidth}
%      \includegraphics[width=\textwidth]{figs/solvers-support-zoom2.png}
%    \end{subfigure}&
%    \begin{subfigure}[b]{0.20\textwidth}
%      \includegraphics[width=\textwidth]{figs/solvers-support-zoom1.png}
%    \end{subfigure}
%  \end{tabular}
%\caption{\small{Runtime of support computation}}
%\label{fig:runtime}
%\end{figure}
\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{figs/runtimeZ3.png}
  \caption{\small{Runtime of support computation with \texttt{Z3} and \texttt{JSupport}}}\label{fig:runtimez3}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{figs/runtimeAll.png}
  \caption{\small{Runtime of support computation with all solvers}}\label{fig:runtimeall}
\end{figure}

\vspace{6pt}
\noindent\fbox{%
    \parbox{\textwidth}{%
        Time-efficiency of computing support set in \texttt{JKind} is not quite solver-dependent. However, SMTInterpol works less efficiently in comparison with others. 
    }%
}
\noindent\fbox{%
    \parbox{\textwidth}{%
        Support computation by \texttt{JSupport} is much more time-intensive than \texttt{reduce-support} engine.
    }%
}
 \vspace{9pt}