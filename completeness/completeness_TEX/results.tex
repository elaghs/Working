\subsection{Results}
\label{sec:results}

\newcommand{\takeaway}[1]{
\vspace{6pt}
\noindent\fbox{\parbox{0.98\columnwidth}{#1}}
\vspace{6pt}
}

\begin{figure*}
  \centering
  \includegraphics[width=0.85\textwidth]{figs/timing_analyses_all_sorted.png}
  \vspace{-0.1in}
  \caption{Runtime of different analyses}\label{fig:runtimeall}
\end{figure*}

\begin{figure*}
  \centering
  \includegraphics[width=0.85\textwidth]{figs/size.png}
  \vspace{-0.1in}
  \caption{Size of the set of covered elements by different algorithms}\label{fig:size}
\end{figure*}

In this section, we examine our experimental results to address the research questions stated in \ref{sec:experiments}.

\textbf{RQ1:} First we examine the performance overhead of the \ucalg algorithm over the time necessary to find a proof using inductive model checking. To measure the performance overhead of an algorithm, we executed it over the proof generated by the {\em fastest} \texttt{JKind} option. Tables~\ref{tab:runtime-ucalg}
and~\ref{tab:overhead-ucalg} represent both the
computation time for coverage analysis
and the overhead imposed by the algorithms.

\begin{table}
  \caption{runtime of different coverage analyses}
  \centering
  \begin{tabular}{ |c||c|c|c|c| }
    \hline
     runtime (sec) & min & max & mean & stdev \\[0.5ex]
    \hline\hline
    %proof time   & 0.047 & 14.617 & 1.299 & 1.940 \\[0.5ex]
    \ucalg &   0.0  & 1.422  & 0.084 & 0.184 \\[0.5ex]
    \mustalg & 0.14 & 997.386 &  19.342 & 97.818 \\[0.5ex]
    \ucbfalg& 0.248 & 1323.515 &  17.247 & 104.838 \\[0.5ex]
    \hline
  \end{tabular} \\
  \label{tab:runtime-ucalg}
\end{table}

\begin{table}
  \caption{Overhead of different coverage analyses}
  \centering
  \begin{tabular}{ |c||c|c|c|c| }
    \hline
     Algorithm & min & max & mean & stdev \\[0.5ex]
    \hline
    \small{\ucalg} &   0.0\%  & 100\%  & 10.23\% & 11.72\% \\[0.5ex]
    \small{\mustalg} & 13.73\% & 10530.77\% &  1081.10\% & 1613.26\% \\[0.5ex]
    \small{\ucbfalg}& 14.09\% & 11124.43\% &  882.02\% & 1512.07\% \\[0.5ex]
    \hline
  \end{tabular}
  \label{tab:overhead-ucalg}
\end{table}

Figure~\ref{fig:runtimeall} allows a visualization of the runtime of different coverage analyses
in comparison with the proof time, which indicates the overhead induced by each algorithm.
As can be seen, it is computationally cheap to find an
approximately minimal IVC using the algorithm \ucalg; however, finding a {\em guaranteed}
minimal IVC using the \ucbfalg\ algorithm is computationally expensive. The overhead of the \ucalg\ algorithm is on average 10\% over the baseline proof, as opposed to 882\% for the \ucbfalg\ algorithm.  
Therefore, in order to compute \ivccov\, it is much more efficient to use \ucalg rather than the \ucbfalg algorithm. 

In terms of comparing cost of coverage computation from \ivccov\ and \mustcov , 
the \mustcov\ computation imposes 1000\% runtime overhead on the verification time, 
while the overhead of \ivccov\ is 10\% on average. 

\takeaway{Coverage analysis using \ucalg is much more efficient than coverage analysis using \mustalg or \ucbfalg.}

\begin{table}
  \caption{Coverage score of different algorithms}
  \centering
  \begin{tabular}{ |c||c|c|c|c| }
    \hline
     score & min & max & mean & stdev \\[0.5ex]
    \hline\hline
    \ucalg &   0.002  & 1.0  &  0.466 & 0.302 \\[0.5ex]
    \mustalg & 0.002 & 1.0 &  0.414 & 0.290 \\[0.5ex]
    \ucbfalg& 0.002 & 1.0 &  0.429 & 0.288 \\[0.5ex]
    \hline
  \end{tabular}
  \label{tab:cov-score}
\end{table}

\textbf{RQ2} and \textbf{RQ3:} When a coverage metric brings about lower coverage scores on average,
it is said that metric is harder to satisfy. In the second research question,
we are interested in comparing this aspect of the proposed metrics.
We first calculated the size of the output sets generated by each algorithm: on average, the ratio of the size of the sets generated by \ucalg to the size of the ones obtained from \ucbfalg is 1.104,
while this ratio for \mustalg to \ucbfalg is 0.958, which shows \mustalg is harder to satisfy, and also is not proof-preserving. 

Figure \ref{fig:size} is a visualization of the size of the set of covered elements by different algorithms. Models over the x-axis are sorted based on the size of MIVCs obtained from the \ucbfalg 
algorithm, which is accurate and matches the exact definition of \ivccov .
The graph shows the degree of under-approximation by \mustalg as well as the degree of over-approximation by \ucalg. 
Moreover, as it can be seen, size of sets computed by \ucalg is very close to the size 
of the ones obtained from \ucbfalg , especially for the larger models. 
In the industrial cases, the size of sets obtained from \ucbfalg and \ucalg is more or less the same, which means the \ucalg is more likely to find MIVCs in realistic problems.
  The average increase in size of IVCs returned by \ucalg\ is approximately 10\% of the \ucbfalg\ algorithm, which makes \ucalg a reasonable choice for computing \ivccov ~(rather than using \ucbfalg ).
Therefore, minimality does not dramatically 
affect the coverage scores when \ivccov\ is computed by the \ucalg\ rather than \ucbfalg. 
%However, \ucalg might report some elements as covered, while they are not because of the minimality issue.
%And, \mustalg reports some elements uncovered, while they are because it is not able to find \emph{may} elements.

Next, we provide a report on the coverage score of the analyses in Table~\ref{tab:cov-score}. In addition, to investigate
the relationship between provability and different coverage notions,
we were interested in the number of models in the benchmark for which
\mustalg resulted in the sets not equal to an MIVC (i.e. models for which
\mustalg did not preserve provability).
Obviously properties are provable by 100\% of the IVCs computed by \ucalg (and \ucbfalg).
As for the \mustalg algorithm, the properties of 84 models in the benchmarks were not provable by the output of \mustalg.

\takeaway{On average, coverage analysis using \ucalg is easier to satisfy,
compared to \mustalg.
%In the assessment of completeness, \ucalg has an average error rate of $+8\%$,
%while the average error rate of \mustalg is $-3\%$.
}

\takeaway{\mustalg failed to maintain provability on 18\% of the benchmarks.} 

\takeaway{The coverage scores computed for the \ivccov\ metric 
are not significantly affected by the minimality issue of the \ucalg algorithm; i.e.
algorithms \ucalg and \ucbfalg yield approximately the same coverage scores for the \ivccov ~metric.} 
