\subsection{Discussion}
\label{sec:discussion}

As we described in Section \ref{sec:background}, transition relation is considered
as the conjunction of Boolean formulae.   The granularity of these formulas substantially affects the analysis results.  For example, in our running example in Section~\ref{sec:illust}, it was possible to have a ``complete'' specification of the model involving only the hysteresis property \hystp.  How could this be?  The way that the model was structured, in order to reach the hysteresis property, we had to ``fall through'' the equations (7) and (8) to reach the hysteresis case (9), so the property depended on these equations.  Furthermore, to evaluate the conditions of (7-8), the other equations (1-6) were required.  The expressions in the description that were {\em not} required were the values assigned to the DOI in the 'then' branches of (7) and (8).  These values could be changed without affecting the correctness of the requirement.  In order to determine that these assignments are irrelevant, it is necessary to analyze the models at a finer granularity.  This can be accomplished by splitting the conjuncts (or equations, in our example) into smaller pieces.  By adding equations for the assignments of `true' and `false' on the then branches of (7) and (8) we can determine that our set of requirements is incomplete.

%Splitting a model into more conjuncts will make coverage scores more accurate and usually lower, though it will not always lower coverage scores.
%
We have recently implemented a transformation that we believe splits models into ``sufficiently granular'' conjuncts such that further decomposition will not cause a complete specification to become incomplete.  We will document this transformation and provide a proof of completeness result preservation in future work.

In a small initial experiment involving 30 of the original models, we performed our transformation and re-ran the analysis.  By changing the granularity of the model, the analysis tools perform significantly slower for proofs, but the ratio of performance between the proof and the \ucalg\ and \nondetcov\ models is largely unchanged.  However, on some models, the \nondetcov\ metric becomes unacceptably slow (analysis times of 10s of hours) and occasionally causes the solver to run out of memory.

The issue of granularity of models is significant, but not discussed in detail in previous coverage metrics.  This will be a focus of our future work, especially in analyzing situations in which the tool determines that a set of requirements is {\em complete}.  

%We plan to focus on efficient analysis of sufficiently granular models in future work.
%when the tool returns that the set of requirements are complete.
