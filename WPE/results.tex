\subsection{Results}
\label{sec:results}

\newcommand{\takeaway}[1]{
\vspace{6pt}
\noindent\fbox{\parbox{0.98\columnwidth}{#1}}
\vspace{6pt}
}

\begin{figure*}
  \centering
  \includegraphics[width=0.85\textwidth]{figs/timing_analyses_all_sorted.png}
  \vspace{-0.1in}
  \caption{Runtime of different analyses}\label{fig:runtimeall}
\end{figure*}

\begin{figure*}
  \centering
  \includegraphics[width=0.85\textwidth]{figs/size.png}
  \vspace{-0.1in}
  \caption{Size of the set of covered elements by different algorithms}\label{fig:size}
\end{figure*}

In this section, we examine our experimental results to address the research questions stated in \ref{sec:experiments}.

\textbf{RQ1:} First we examine the performance overhead of the \ucalg algorithm over the time necessary to find a proof using inductive model checking. To measure the performance overhead of an algorithm, we executed it over the proof generated by the {\em fastest} JKind option. Tables~\ref{tab:runtime-ucalg}
and~\ref{tab:overhead-ucalg} represent both the
computation time for coverage analysis
and the overhead imposed by the algorithms.

\begin{table}
  \caption{runtime of different coverage analyses}
  \centering
  \begin{tabular}{ |c||c|c|c|c| }
    \hline
     runtime (sec) & min & max & mean & stdev \\[0.5ex]
    \hline\hline
    %proof time   & 0.047 & 14.617 & 1.299 & 1.940 \\[0.5ex]
    \ucalg &   0.0  & 1.422  & 0.084 & 0.184 \\[0.5ex]
    \mustalg & 0.14 & 997.386 &  19.342 & 97.818 \\[0.5ex]
    \ucbfalg& 0.248 & 1323.515 &  17.247 & 104.838 \\[0.5ex]
    \hline
  \end{tabular} \\
  \label{tab:runtime-ucalg}
\end{table}

\begin{table}
  \caption{Overhead of different coverage analyses}
  \centering
  \begin{tabular}{ |c||c|c|c|c| }
    \hline
     Algorithm & min & max & mean & stdev \\[0.5ex]
    \hline
    \small{\ucalg} &   0.0\%  & 100\%  & 10.23\% & 11.72\% \\[0.5ex]
    \small{\mustalg} & 13.73\% & 10530.77\% &  1081.10\% & 1613.26\% \\[0.5ex]
    \small{\ucbfalg}& 14.09\% & 11124.43\% &  882.02\% & 1512.07\% \\[0.5ex]
    \hline
  \end{tabular}
  \label{tab:overhead-ucalg}
\end{table}

Fig.~\ref{fig:runtimeall} allows a visualization of the runtime of different coverage analyses
in comparison with the proof time, which indicates the overhead induced by each algorithm.

\takeaway{Coverage analysis using \ucalg is much more efficient than coverage analysis using \mustalg or \ucbfalg.}

\begin{table}
  \caption{Coverage score of different algorithms}
  \centering
  \begin{tabular}{ |c||c|c|c|c| }
    \hline
     score & min & max & mean & stdev \\[0.5ex]
    \hline\hline
    \ucalg &   0.002  & 1.0  &  0.466 & 0.302 \\[0.5ex]
    \mustalg & 0.002 & 1.0 &  0.414 & 0.290 \\[0.5ex]
    \ucbfalg& 0.002 & 1.0 &  0.429 & 0.288 \\[0.5ex]
    \hline
  \end{tabular}
  \label{tab:cov-score}
\end{table}

\textbf{RQ2:} When a coverage metric brings about lower coverage scores on average,
it is said that metric is harder to satisfy. In the second research question,
we are interested in comparing this aspect of the proposed metrics.
We first calculated the size of the output sets generated by each algorithm: on average, the ratio of the size of the sets generated by \ucalg to the size of the ones obtained from \ucbfalg is 1.104, 
while this ratio for \mustalg to \ucbfalg is 0.958, which shows \mustalg is harder to satisfy, and also does not maintain provability. Fig. \ref{fig:size} is a visualization of the size of the set of covered elements by different algorithms. The graph shows the degree of under-approximation by \mustalg as well as the degree of over-approximation by \ucalg.
Note that \ucbfalg is accurate and matches the exact definition of \ivccov .
%However, \ucalg might report some elements as covered, while they are not because of the minimality issue.
%And, \mustalg reports some elements uncovered, while they are because it is not able to find \emph{may} elements.

Next, we provide a report on the coverage score of the analyses in Table~\ref{tab:cov-score}. In addition, to investigate
the relationship between provability and different coverage notions,
we were interested in the number of models in the benchmark for which
\mustalg resulted in the sets not equal to an IVC (i.e. models for which
\mustalg did not preserve provability).
Obviously properties are provable by 100\% of the IVCs computed by \ucalg (and \ucbfalg).
As for the \mustalg algorithm, the properties of 84 models in the benchmarks were not provable by the output of \mustalg.

\takeaway{On average, coverage analysis using \ucalg is easier to satisfy,
compared to \mustalg.
%In the assessment of completeness, \ucalg has an average error rate of $+8\%$,
%while the average error rate of \mustalg is $-3\%$.
}

\takeaway{\mustalg failed to maintain provability on 18\% of the benchmarks.} 