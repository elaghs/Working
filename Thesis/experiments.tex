\chapter{Experiment}
\label{ch:experiment}

\newcommand{\takeaway}[1]{
\vspace{12pt}
\noindent\fbox{\parbox{\textwidth}{#1}}
\vspace{6pt}
}
We would like to evaluate different algorithms presented in Chapter \ref{ch:ivc}. 
 The use of \texttt{JKind} allows additional dimensions to our investigation: as mentioned before, it supports two different inductive algorithms: $k$-induction and PDR, and a ``fastest'' mode, that runs both algorithms in parallel.  Also, \texttt{JKind} supports multiple back-end SMT solvers including \texttt{Z3}~\cite{DeMoura08:z3}, \texttt{Yices}~\cite{Dutertre06:yices}, \texttt{MathSAT}~\cite{Cimatti2013:MathSAT}, and \texttt{SMTInterpol}~\cite{Christ2012:SMTInterpol}.
 
 This chapter is organized in two separate parts; first we present an initial evaluation to address a basic question for IVC computation: we would like to determine whether the choice of inductive algorithm affects the size of the IVCs calculated by \ucalg , whether different solvers are more or less efficient at producing approximate minimal IVCs (\ucalg), and whether running different solvers/algorithms leads to {\em diversity} of IVC solutions obtained by \ucalg\ (Section \ref{sec:exprinit}).

The initial experiments allow us to choose an optimal configuration for \texttt{JKind} based on which we can run our major experiments. The major experiments are conducted to evaluate the efficiency and efficacy the IVC algorithms. Section \ref{sec:exp1} is about the evaluation of our two key proposed algorithms: \ucalg\ and \aivcalg . Then in Section \ref{sec:exp2}, we examine our approach for calculating all minimal IVCs in the online manner as described in \ref{sec:onaivc}. Finally in Section \ref{sec:exp3}, we investigate a couple of interesting research questions related to the bounded validity cores.

\section{Initial Experiments}
\label{sec:exprinit}
For the purpose of initial experiments, we started from a suite of 700 Lustre models developed
as a benchmark suite for~\cite{Hagen08:FMCAD}. We augmented this suite
with 81 additional models from recent verification projects including
avionics and medical devices~\cite{QFCS15:backes,hilt2013}. Most of
the benchmark models from~\cite{Hagen08:FMCAD} are small (10kB or less,
with 6-40 equations) and contain a range of hardware benchmarks and
software problems involving counters. The additional models are much
larger: around 80kB with over 300 equations. We added the new
benchmarks to better check the scalability for the tools, especially
with respect to the brute force algorithm.
%
%\mike{MORE HERE...stats on size, reasons for add'l models.}
Each benchmark model has a single property to analyze.  For our purposes, we are only interested in models with a {\em valid} property (though it is perhaps worth noting that there is no additional computation---and thus no overhead---using the \texttt{JKind} IVC options for {\em invalid} properties).  In our benchmark set, 295 models yield counterexamples, and 10 additional models are neither provable nor yield counterexamples in our test configuration (see next paragraph for configuration information).  The benchmark suite therefore contains 476 models with valid properties, which we use as our initial test subjects.

For each test model, we computed \ucalg\ in 12+1 configurations: the
twelve configurations were the cross product of all solvers \{\texttt{Z3},
\texttt{Yices}, \texttt{MathSAT}, \texttt{SMTInterpol}\} and inductive algorithms
\{$k$-induction, PDR, fastest\}, and the remaining (+1) configuration
was an instance of \bfalg run on \texttt{Yices}, which is the default solver in
JKind.
 In addition, for each of the 12 configurations, we ran an
instance of \texttt{JKind} without IVC to examine overhead. The initial experiments
were run on an Intel(R) i5-2430M, 2.40GHz, 4GB memory machine, with a
1 hour timeout for each analysis on any model. The data gathered for
each configuration of each model included the time required to check
the model without IVC, with IVC, and also the set of elements in the
computed IVC.\footnote{The benchmarks, all raw experimental results,
  and computed data are available on \cite{expr}.}

Note that not all analysis problems were solvable with all algorithms: for all solvers, $k$-induction (without IVC) was unable to solve 172 of the examples.  When comparing minimality of different solving algorithms, we only considered cases where both algorithms provided a solution.

For this study, we are interested in the following research questions:
\begin{itemize}
  \item \textbf{RQ1:} Does the choice of SMT solver affect the performance of \ucalg ?
  \item \textbf{RQ2:} Does the choice of SMT solver or algorithm used to produce a proof
(k-induction or PDR) matter in terms of the minimality of the IVCs generated by \ucalg ?
  \item \textbf{RQ3:} Do different solvers and algorithms
lead to different minimal cores for \ucalg ?
\end{itemize}

\vspace{0.1in}
\subsubsection{RQ1}
First we examine the performance overhead of the \ucalg\ algorithm over the time necessary to find a proof using inductive model checking.  To examine this question, we use the default {\em fastest} option of JKind which terminates when either the $k$-induction or PDR algorithm finds a proof.  To measure the performance overhead of the \ucalg\ algorithm, we execute it over the proof generated by the {\em fastest} option.

Since the \ucalg\ algorithm uses the UNSAT core facilities of the
underlying SMT solver, the performance is dependent on the efficiency
of this part of the solver. Looking at Tables~\ref{tab:runtime-ucalg-solvers}
and~\ref{tab:overhead-ucalg-solvers}, it is possible to examine both the
computation time for analysis using the four solvers under evaluation
and the overhead imposed by the \ucalg\ algorithm.
Figure~\ref{fig:perf-solvers} allows a visualization of the runtime for
the \ucalg\ algorithm running different solvers. The data suggests that
\texttt{Yices} (the default solver in \texttt{JKind}) and \texttt{Z3} are the most performant
solvers both in terms of computation time and overhead.

\begin{figure*}
  \centering
  \includegraphics[width=\textwidth]{figs/performance_solvers.png}
    \vspace{-0.1in}
  \caption{\ucalg\ performance on different solvers} 
  \label{fig:perf-solvers}
\end{figure*}

\begin{table}
  \caption{\ucalg runtime with different solvers}
  \centering
  \begin{tabular}{ |c||c|c|c|c| }
    \hline
     runtime (sec) & min & max & mean & stdev \\[0.5ex]
    \hline\hline
 %   JSupport & 2.381 & 165.157 & 21.533 & 23.533 \\[0.5ex]
    Z3   & 0.005 & 2.335 & 0.192 & 0.355 \\[0.5ex]
    Yices &   0.014  & 13.297   & 0.589 & 1.473 \\[0.5ex]
    SMTInterpol& 0.029 & 19.254 &  1.396 & 2.991 \\[0.5ex]
    MathSAT & 0.011 & 86.421 &  3.071 & 10.403 \\[0.5ex]
    \hline
  \end{tabular} \\
  \label{tab:runtime-ucalg-solvers}
\end{table}

\begin{table}
  \caption{Overhead of \ucalg computations using different solvers}
  \centering
  \begin{tabular}{ |c||c|c|c|c| }
    \hline
     solver & min & max & mean & stdev \\[0.5ex]
    \hline
    Z3   & 0.73\% & 84.13\% & 17.38\% & 16.92\% \\[0.5ex]
    Yices &   0.17\%  & 351.47\%   & 52.20\% & 54.50\% \\[0.5ex]
   SMTInterpol& 1.46\% & 175.75\% &  46.81\% & 37.35\%\\[0.5ex]
    MathSAT & 0.78\% & 955.52\% &  80.21\% & 112.92\%\\[0.5ex]
    \hline
  \end{tabular}
  \label{tab:overhead-ucalg-solvers}
\end{table}

\vspace{0.1in}
\subsubsection{RQ2}
As described in Chapter~\ref{ch:ivc}, the \ucalg\
algorithm is not guaranteed to produce minimal cores due in part to
the role of invariants used in producing a proof; as $k$-induction and
PDR use substantially different invariant generation algorithms, it is
likely that the set of necessary invariants for proofs are dissimilar,
and that this would in turn affect the number of model elements required for
the proof.  It is possible that one or the other algorithm is more likely
to yield smaller invariant sets.  In addition, differences in the choice of the
UNSAT core algorithms in the different solvers could affect the size of the
generated core. However, our algorithm already performs a minimization
step on UNSAT cores, and thus the only differences would be due to one
algorithm leading to a different minimal core than another.

As mentioned, $k$-induction is unable to solve all of the analysis problems; therefore we include only models that are solvable using {\em both} $k$-induction and PDR by {\em all solvers}, 304 models in all.  Examining the aggregate data in Table~\ref{tab:minimality-algorithm-solvers}, we can see the sizes of cores produced by different algorithms and solvers.
\vspace{0.1in}
\subsubsection{RQ3}
In this section, we examine the issue of diversity:
do different solvers and algorithms lead to {\em different} minimal
cores? This is both a function of the models and the solution
algorithms: for certain models, there is only one possible minimal IVC
set, whereas other models might have many. Given that there are
multiple solutions, the interesting question is whether using
different solvers and algorithms will lead to different solutions.
The reason diversity is considered is that it has substantial relevance to
some of the uses of the tool, e.g., for constructing multiple traceability
matrices from proofs (see Section~\ref{sec:traceability}).
Note that our exploration in this experiment is not
exhaustive, but only exploratory, based on the IVCs returned by different
algorithms and tools; we leave exhaustive exploration of
IVCs for future work.

%Given diversity of results, we may wish to
%distinguish {\em must} traceability elements from {\em may}
%traceability elements across a set of diverse solutions, and consider
%more systematic explorations of diversity in future work.

To measure diversity of the generated IVCs, we use Jaccard distance:
\begin{definition}{\emph{Jaccard distance:}}
  \label{def:dj}
  $d_J(\small{A}, \small{B}) = 1 - \frac{|A \cap B|}{|A \cup B|} ,\\ 0 \leq d_J(\small{A}, \small{B}) \leq 1$
\end{definition}
\noindent Jaccard distance is a standard metric for comparing finite
sets (assuming that both sets are non-empty) by comparing the size of
the intersection of two sets over its union. For each model in the
benchmark, the experiments generated 13 potentially different IVCs. Therefore, we
obtained $\binom{13}{2} = 78$ combinations of pairwise distances per
model. Then, minimum, maximum, average, and standard deviation of the
distances were calculated (Figure~\ref{fig:jacdis}), by which, again,
we calculated these four measures among all models. As seen in
Table~\ref{tab:jaccard-avg}, on average, the Jaccard distance between
different solutions is small, but the maximum is close to 1, which
indicates that even for our exploratory analysis, there are models for
which the tools yield substantially diverse solutions. The diversity
between solutions is represented graphically in
Figure~\ref{fig:jacdis}, where for each model, we present the min,
max, and mean pairwise Jaccard distance of the solutions produced by algorithm
\ucalg for each model, ranked by the mean distance.

\begin{table}
  \caption{Pairwise Jaccard distances among all models}
  \centering
  \begin{tabular}{ |c|c|c|c| }
    \hline
     min & max & mean & stdev \\[0.5ex]
    \hline
    %sample size = 4196
     0.0   & 0.878 & 0.026 & 0.059 \\[0.5ex]
    \hline
  \end{tabular}
  \label{tab:jaccard-avg}
\end{table}

\begin{figure*}
  \centering
  \vspace{3mm}
  \includegraphics[width=\textwidth]{figs/jacdis2.png} \\
  \vspace{-0.1in}
  \caption{Pairwise Jaccard distance between IVCs}\label{fig:jacdis}
\end{figure*}

\iffalse 

To measure the overall similarity among all sets, instead of a pairwise comparison, we used \emph{frequent pattern mining} \cite{han2007frequent}. To define an overall similarity among all sets of support of a given model\footnote{Note that all models in the benchmarks are single property; hence, instead of saying a set of support of a given \emph{property}, we just refer it as the support set of the \emph{model} while explaining the experimental results.}, we calculated a \emph{core} support set for each model in the benchmark, which can be considered as a closed frequent pattern; a core set of model $M$, denoted by $C_M$, is defined as:
\begin{definition}
  \label{def:core}
  $C_M = \bigcap_{i=1}^{13} s_{Mi},   \hspace{9pt} s_{Mi} \in S_M$
\end{definition}

Based on this notion, overall dissimilarity, denoted by $D_{J\{M\}}$, is defined as follows:

\begin{definition}
  \label{def:dis}
  $D_{J\{M\}} =  \frac{\sum_{i=1}^{12}d_J(s_{Mi}, C_M)}{12},   \hspace{9pt} s_{Mi} \in S_M$
\end{definition}

Since our goal is to measure the diversity or dissimilarity among sets computed by \texttt{ReduceSupport}, in \ref{def:dis}, we exclude the set generated by \ucbfalg . In Fig~\ref{fig:jacdis}, the \emph{overall distance} line shows $D_{J\{M\}}$ per model, which can be analyzed from the following hypotheses:
\begin{itemize}
  \item H0: variety of obtained sets of support is high (average $D_{J\{M\}}$ of 0.2)
  \item H1: variety of obtained sets of support is small (average $D_{J\{M\}}$ less than 0.2)
\end{itemize}
Table~\ref{tab:variety} shows that, with an effect size of 0.79, H0 can be rejected.
\begin{table}
  \caption{$D_{J\{M\}}$ among all models}
  \centering
  \begin{tabular}{ |c|c|c|c|c|c| }
    \hline
     min & max & mean & stdev & ES & p-value\\[0.5ex]
    \hline
    %sample size = 395
     0.0   & 0.879 & 0.099 & 0.141 & 0.72 & < 0.00001 \\[0.5ex]
    \hline
  \end{tabular}
  \label{tab:variety}
\end{table}
\fi
%summarized as follows:
%\begin{itemize}
%  \item minimum $D_{J\{M\}}$ among all models: 0.0
%  \item maximum $D_{J\{M\}}$ among all models: 0.879
%  \item average $D_{J\{M\}}$ among all models: 0.096
%  \item standard deviation of $D_{J\{M\}}$ among all models: 0.132
%\end{itemize}
%if we sum all elements of all cores together, that PDR has an smaller core size in aggregate than $k$-induction.
%However, the data is noisy, and to examine \textbf{RQ2.1} systematically, we construct a hypothesis that PDR will, in general, equal or outperform $k$-induction on an arbitrary model:

%\mike{ADD HYPOTHESIS/NULL HYPOTHESIS HERE}

%Although the aggregate data suggests that PDR will yield a smaller core (on average) than $k$-induction, this claim is not supported for a given model with significance.

%we already perform a linear scan of the cores generated by the SMT solver to remove unnecessary conjuncts

\begin{table}
  \caption{Aggregate IVC sizes produced by \ucalg\ using different inductive algorithms and solvers}
  \centering
  \begin{tabular}{ |c|c|c|c| }
    \hline
     solver & PDR & $k$-induction & \textbf{total} \\
    \hline
      Z3 & 2378 & 2379 & 4757 \\
      Yices & 2384 & 2376 & 4760 \\
      MathSAT & 2375 & 2369 & 4744 \\
      SMTInterpol & 2378 & 2368 & 4746 \\
    \hline
      \textbf{total} & 9515 & 9492 &   \\
    \hline
  \end{tabular}
  \label{tab:minimality-algorithm-solvers}
\end{table}



\section{Experiments on the IVC\_UC and ALL\_IVCs Algorithms}
\label{sec:exp1}
\input{expr1}

\section{Experiments on the Online method of computing AIVCs}
\label{sec:exp2}
\input{expr}


\section{Experiments on BVCs}
\label{sec:exp3}
\input{bvcexp}
