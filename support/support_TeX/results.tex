\section{Results}
\label{sec:results}

\newcommand{\takeaway}[1]{
\vspace{6pt}
\noindent\fbox{\parbox{\columnwidth}{#1}}
\vspace{6pt}
}

In this section, we examine our experimental results from three perspectives: performance, minimality of \ucalg results and diversity.

\iffalse
\mike{Results and Discussion:}
\begin{itemize}
    \item (Overview) Give an overview of results, possibly referencing a graph or two.
    \item Statistical analysis: provide positive and null hypotheses for the research questions.
    \item Evaluation of research questions: statistical results and explanations of the graphs.  One subsection per research question?
    \item Threats to validity: what are our threats?
    Internal validity: not really necessary, I think.
    External validity (how much can the results be generalized):
        1. currently all models are *very small*;
        2. Many programs were drawn from mutations of a relatively small number of ``seed'' programs;
        3. The models are written in Lustre rather than FOL.  This means that the
            top-level conjunctions are all over equations rather than general
            form;
        4. Others?!?
    Construct validity: we are measuring what we think we're measuring: IVC and minimality are reasonably defined.  For discussions of ``completeness'' and ``traceability'' we need to be clear about any claims (probably not in this paper).
\end{itemize}
\fi

\subsection{Performance}
\label{sec:performance}

In this subsection, we examine the performance of our inductive validity core algorithms (research question \textbf{RQ1}).  First we examine the performance overhead of the \ucalg algorithm over the time necessary to find a proof using inductive model checking.  To examine this question, we use the default {\em fastest} option of JKind which terminates when either the k-induction or PDR algorithm finds a proof.  To measure the performance overhead of the \ucalg algorithm, we execute it over the proof generated by the {\em fastest} option.

Since the \ucalg algorithm uses the UNSAT-core facilities of the underlying SMT solver, the performance is dependent on the efficiency of this part of the solver.  Examining Tables~\ref{tab:runtime-ucalg} and~\ref{tab:overhead-ucalg}, it is possible to examine both the aggregate computation time for analysis using the four solvers under evaluation and the overhead imposed by the \ucalg algorithm.  The data suggests that yices (the default solver in JKind) and z3 are the most performant solvers both in terms of computation time and overhead.  Figure~\ref{fig:runtimeall} allows a visualization of the overhead for the \ucalg algorithm running different tools (as well as the \bfalg, discussed below).  For this figure, models are ranked on the x-axis in terms of their analysis time using the z3 solver to perform the proof without performing IVC.

%Figure~\ref{fig:runtimez3} allows a visualization of the overhead for the Z3 solver, where the models are ranked on the x axis in terms of their analysis time without performing \ucalg, and the y-axis describes analysis time with and without \ucalg.

%\mike{Add the raw timings for each solver for proof and proof + \ucalg analysis in Table~\ref{tab:overhead}.}

%Although it is relatively obvious from Table~\ref{tab:overhead}, it is straightforward to demonstrate with statistical significance that Z3 outperforms other solvers.  The hypotheses are as follows: \mike{FILL IN HYPOTHESES}.

%\mike{Do we need hypotheses here?  We could say a 'moderate' performance penalty is under 50\% over the regular solver time}.

\takeaway{The \ucalg algorithm using the Z3 and yices SMT solvers adds a modest performance penalty to the time required for inductive proofs.}

Next, we consider the overhead of \ucalg vs. \bfalg.  Recall from Section~\ref{sec:ivc} that \bfalg requires $n$ model checking runs, where $n$ is the number of conjuncts in the transition relation. As expected, the performance is approximately a linear multiple of the size of the model, so larger models yield substantially lower performance.\footnote{for Lustre models, the number of conjuncts is equivalent to the number of equations in the Lustre model.}  We run the brute-force algorithm using yices as it is the default solver for JKind and is very close to z3 in terms of computation time.  For \mike{YYY} models, \bfalg times out after 1 hour.   Figure~\ref{fig:runtimeall} shows the overhead of \bfalg in comparison to \ucalg with multiple solvers.

\takeaway{The brute-force algorithm \bfalg adds a substantial performance penalty to inductive proofs in all cases and is not scalable enough to compute a minimal core for large analysis problems.}

Finally, we consider the combined \ucbfalg algorithm, in which we first run the \ucalg to determine a close-to-minimal set of support, then run the \bfalg on the remaining set.  The overhead of this algorithm is considered in Tables~\ref{tab:runtime-ucbfalg} and~\ref{tab:overhead-ucbfalg}.  While considerably slower than the \ucalg, this approach can still be used for reasonably sized models.

%\mike{Fill in tables HERE!}
\begin{figure*}
  \centering
  \includegraphics[width=\textwidth]{figs/runtimeAll.png}
  \mike{UPDATE TO INCLUDE ALL MODELS (MICROWAVE AND GPCA)}
  \mike{There are 4 spelling errors in the key: `wihtout' rather than `without'.  Also, what provides the ordering in this figure?  Is it z3 running without IVC?}
  \mike{Use logarithmic scale to better show differences!}
  \mike{Compress height a little bit.}
  \mike{Can we add \ucbfalg to this graph?}
  \caption{Runtime of \ucalg computation for all solvers and \bfalg, \ucbfalg algorithm for Yices}\label{fig:runtimeall}
\end{figure*}


\begin{table}
  \centering
  \begin{tabular}{ |c||c|c|c|c| }
    \hline
     runtime (sec) & min & max & mean & stdev \\[0.5ex]
    \hline\hline
 %   JSupport & 2.381 & 165.157 & 21.533 & 23.533 \\[0.5ex]
    Z3   & 0.112 & 42.928 & 2.412 & 5.009 \\[0.5ex]
    Yices &   0.111  & 39.657   & 2.464 & 5.224 \\[0.5ex]
    SMTInterpol& 0.225 & 514.886 &  4.331 & 26.411 \\[0.5ex]
    MathSAT & 0.111 & 43.623 &  2.765 & 5.157 \\[0.5ex]
    \hline
  \end{tabular} \\ 
  \mike{Is this the time for performing both the proof and performing the \ucalg algorithm or is it just the time for the \ucalg algorithm?}
  \caption{\ucalg runtime with different solvers}
  \label{tab:runtime-ucalg}
\end{table}

\begin{table}
  \centering
  \begin{tabular}{ |c||c|c|c|c| }
    \hline
     solver & min & max & mean & stdev \\[0.5ex]
    \hline
    Z3   & 0.726\% & 45.396\% & 13.414\% & 11.369\% \\[0.5ex]
    Yices &   0.200\%  & 262.254\%   & 47.264\% & 51.193\% \\[0.5ex]
    SMTInterpol& 0.930\% & 268.571\% &  70.500\% & 58.541\%\\[0.5ex]
    MathSAT & 0.502\% & 396.124\% &  71.007\% & 79.990\%\\[0.5ex]
    \hline
  \end{tabular}
  \caption{Overhead of \ucalg computations using different solvers}
  \label{tab:overhead-ucalg}
\end{table}

\begin{table}
  \centering
  \begin{tabular}{ |c||c|c|c|c| }
    \hline
     runtime (sec) & min & max & mean & stdev \\[0.5ex]
    \hline\hline
    Z3   & ??? & ??? & ??? & ??? \\[0.5ex]
    Yices &   ???  & ???   & ??? & ??? \\[0.5ex]
    \hline
  \end{tabular}
  \caption{\ucbfalg runtime with Z3 and Yices}
  \label{tab:runtime-ucbfalg}
\end{table}

\begin{table}
  \centering
  \begin{tabular}{ |c||c|c|c|c| }
    \hline
     solver & min & max & mean & stdev \\[0.5ex]
    \hline
    Z3   & XXX\% & XXX\% & XXX\% & XXX\% \\[0.5ex]
    Yices &   XXX\%  & XXX\%   & XXX\% & XXX\% \\[0.5ex]
    \hline
  \end{tabular}
  \caption{Overhead of \ucbfalg algorithm using Z3 and Yices}
  \label{tab:overhead-ucbfalg}
\end{table}


\begin{table}
  \centering
  \begin{tabular}{ |c|c|c| }
    \hline
     brute-force & PDR & k-induction \\
    \hline
      XX & XX & XX  \\
    \hline
  \end{tabular}
  \caption{Aggregate IVC sizes produced by \ucalg using different inductive algorithms}
  \label{tab:minimality-algorithm}
\end{table}

\begin{table}
  \centering
  \begin{tabular}{ |c|c|c| }
    \hline
     solver & aggregate & \% increase over minimal \\
    \hline
    brute-force & 3078 & * \\
    z3 & ?? & XX\% \\
    yices & ?? & YY\% \\
    MathSAT & ?? & ZZ\% \\
    SMTInterpol & ?? & AA\% \\
    \hline
  \end{tabular}
  \caption{Aggregate IVC sizes produced by \ucalg using different solvers}
  \label{tab:minimality-solver}
\end{table}

\begin{figure*}
  \centering
  \includegraphics[width=\textwidth]{figs/runtimeAll.png}
  \mike{REPLACE WITH CORRECT FIGURE}
  \caption{IVC sizes produced by \ucalg vs. \ucbfalg for yices solver}
  \label{fig:minimality-all}
\end{figure*}

\subsection{Minimality}
\label{sec:minimality}
In this section, we examine the minimality of the cores computed by the \ucalg algorithm using different inductive proof methods and compare it to the cores produced by the brute-force algorithm (\textbf{RQ2}).  There are three interesting aspects to be examined related to this research question.  First (\textbf{RQ2.1}), does the choice of algorithm used to produce a proof (k-induction or PDR) matter in terms of the minimality of the inductive core?   As mentioned in Section~\ref{sec:ivc}, the \ucalg algorithm is not guaranteed to produce a minimal core due to the role of invariants used in producing a proof; as k-induction and PDR use different invariant generation algorithms, it is possible that one or the other is more likely to yield smaller invariant sets.  In addition, as discussed in Section~\ref{sec:experiment}, k-induction is unable to solve all of the analysis problems; therefore we include only models that are solvable using both k-induction and PDR.  Examining the aggregate data in Table~\ref{tab:minimality-algorithm}, if we sum all elements of all cores together, that PDR has an smaller core size in aggregate than k-induction.  However, the data is noisy, and to examine \textbf{RQ2.1} systematically, we construct a hypothesis that PDR will, in general, equal or outperform k-induction on an arbitrary model:

\mike{ADD HYPOTHESIS/NULL HYPOTHESIS HERE}

Although the aggregate data suggests that PDR will yield a smaller core (on average) than k-induction, this claim is not supported for a given model with significance.

\takeaway{Neither PDR nor k-induction yields a smaller inductive validity core with statistical significance.}

Second (\textbf{RQ2.2}), does the choice of solver used to produce a proof matter in terms of the minimality of the inductive core?   As previously mentioned in Section~\ref{sec:performance}, since the \ucalg algorithm is reliant on the UNSAT-core facilities of the underlying SMT solver, differences in the choice of the UNSAT-core algorithms could affect the size of the generated core.  However, in our current tool, to address known inaccuracies in UNSAT-core generation in all tools, we do an additional step of post-processing the returned cores to ensure that they are minimal through the simple expedient of removing activation literals one by one from the core and checking to see if the query is still UNSAT.  This step is not described in the algorithms in Section~\ref{sec:ivc}, but is performed by our implementation in JKind.  Therefore, the differences in minimality of solutions from solvers is dealt with directly in the implementation algorithm, and so we do not discuss it further here.

%we already perform a linear scan of the cores generated by the SMT solver to remove unnecessary conjuncts

The final relevant question (\textbf{RQ2.3}) asks how close to minimal are the cores produced by \ucalg vs. the (guaranteed minimal) cores produced by the \bfalg algorithm?  Note that we cannot measure the distance on all models because the brute-force algorithm times out on many of the larger models.  We therefore examine the distance from minimal cores produced by the brute force algorithm for models in which it completes within the one hour timeout.  For comparison, we run the \ucalg algorithm in each tool with the default {\em fastest} algorithm, which will use the result of either k-induction or PDR.  The aggregate results are shown in Table~\ref{tab:minimality-solver}, and a graph showing the performance on each model, ranked along the x-axis by the size of the core produced by \bfalg is shown in Figure~\ref{fig:minimality-all}.  The figure demonstrates that while on average there is a modest change in minimality that there can be substantial variance on the sizes of the cores produced by the \ucalg algorithm.

\takeaway{The \ucalg algorithm computes cores that are XX\% to YY\% larger than those produced by \ucbfalg, with substantial variance in the results.}


\iffalse
\ela{I think this is not needed: \\In terms of size, we calculated the size of the biggest and smallest sets per model, then added them together for all models. The same calculation has been done for \texttt{JSupport}:
\begin{itemize}
  \item $A\_JS$: the aggregate number of elements in support sets computed by \texttt{JSupport} = 3078
  \item $A\_Ss$: the aggregate number of elements in the \emph{smallest} support sets = 3474;
  this implies $A\_Ss$ is 12\% greater than $A\_JS$.
  \item $A\_Bs$: the aggregate number of elements in the \emph{biggest} support sets = 3586;
  this implies $A\_Bs$ is 16\% greater than $A\_JS$.
\end{itemize}}
Average size of sets computed by \texttt{ReduceSupport} is 8.55. And, average size of sets computed by \texttt{JSupport} is 7.60. Therefore, in average, support sets computed by \texttt{ReduceSupport} are 88\% close to minimal, in terms of their size.  %  (1 - ((8.55 - 7.6)/ 7.6))  * 100 = 87.5%
Since \texttt{JSupport}, with a great percentage, most of the time computed the smallest support set, we compared the size of the sets computed by \texttt{ReduceSupport} with \texttt{JSupport}. For each configuration, we collected the difference between its support size and \texttt{JSupport} per model. Table~\ref{tab:minimality} shows the result of analysis.
\fi

\begin{table}
  \centering
  \begin{tabular}{ |c|c|c|c|c|c| }
    \hline
     min & max & mean & stdev & ES & p-value\\[0.5ex]
    \hline
    %sample size = 4196
     0.0   & 0.882 & 0.027 & 0.062 & 2.79 & < 0.00001 \\[0.5ex]
    \hline
  \end{tabular}
  \caption{Pairwise Jaccard distances among all models}
  \label{tab:jaccard-avg}
\end{table}

\begin{figure*}
  \centering
  \includegraphics[width=0.75\textwidth]{figs/jacdis2.png} \\ 
  \mike{Add new models, and fix spelling error (mosdels) !}
  \caption{Pairwise Jaccard distance between support sets}\label{fig:jacdis}
\end{figure*}

\subsection{Diversity}
\label{sec:diversity}
Recall from Section~\ref{sec:ivc} that a {\em minimal} core is any core leading to a proof such that if you remove any of the conjuncts from the core, it no longer produces a proof.  For certain models and properties, it is possible that there are many minimal cores that will lead to a proof.  In this section, we examine the issue of diversity: do different tools and algorithms lead to {\em different} minimal cores?  This is both a function of the models and the solution algorithms: for certain models, there is only one possible minimal core, whereas other models might have many. Given that there are multiple solutions, the interesting question is whether using different tools and algorithms will lead to different solutions.  Note that this question is closely tied to that of {\em minimality}, which we examined in the previous section.

Our exploration in this case is not exhaustive, but only exploratory.  The reason it is considered is that it has substantial relevance to some of the uses of the tool: e.g., for constructing traceability matrices from proofs.  Given diversity of results, we may wish to distinguish {\em must} traceability elements from {\em may} traceability elements across a set of diverse solutions, and consider more systematic explorations of diversity in future work.

To measure diversity of IVCs, we use Jaccard distance~\mike{citation?}:
\begin{definition}{\emph{Jaccard distance:}}
  \label{def:dj}
  $d_J(\small{A}, \small{B}) = 1 - \frac{|A \cap B|}{|A \cup B|} ,\\ 0 \leq d_J(\small{A}, \small{B}) \leq 1$
\end{definition}
\noindent Jaccard distance is a standard metric for comparing finite sets (assuming that both sets are non-empty) by comparing the size of the intersection of two sets over its union.  For each model in the benchmark, the experiments generated 13 different sets of support. Therefore, we obtained $\binom{13}{2} = 78$ combinations of pairwise distances per model. Then, minimum, maximum, average, and standard deviation of the distances were calculated (Fig~\ref{fig:jacdis}), by which, again, we calculated these four measures among all 405 models.  As seen in Table~\ref{tab:jaccard-avg}, on average, the Jaccard distance between different solutions is small, but the maximum is close to 1, which indicates that even for our exploratory analysis, there are models for which the tools yield substantially diverse solutions.  The diversity between solutions is represented graphically in Figure~\ref{fig:jacdis}, where for each model, we present the min, max, and mean pairwise distance of the solutions produced by algorithm \ucalg for each model, ranked by the mean distance.


\mike{Do we want this discussion?  I am not sure that it adds much}

To measure the overall similarity among all sets, instead of a pairwise comparison, we used \emph{frequent pattern mining} \cite{han2007frequent}. To define an overall similarity among all sets of support of a given model\footnote{Note that all models in the benchmarks are single property; hence, instead of saying a set of support of a given \emph{property}, we just refer it as the support set of the \emph{model} while explaining the experimental results.}, we calculated a \emph{core} support set for each model in the benchmark, which can be considered as a closed frequent pattern; a core set of model $M$, denoted by $C_M$, is defined as:
\begin{definition}
  \label{def:core}
  $C_M = \bigcap_{i=1}^{13} s_{Mi},   \hspace{9pt} s_{Mi} \in S_M$
\end{definition}

Based on this notion, overall dissimilarity, denoted by $D_{J\{M\}}$, is defined as follows:

\begin{definition}
  \label{def:dis}
  $D_{J\{M\}} =  \frac{\sum_{i=1}^{12}d_J(s_{Mi}, C_M)}{12},   \hspace{9pt} s_{Mi} \in S_M$
\end{definition}

Since our goal is to measure the diversity or dissimilarity among sets computed by \texttt{ReduceSupport}, in \ref{def:dis}, we exclude the set generated by \texttt{JSupport}. In Fig~\ref{fig:jacdis}, the \emph{overall distance} line shows $D_{J\{M\}}$ per model, which can be analyzed from the following hypotheses:
\begin{itemize}
  \item H0: variety of obtained sets of support is high (average $D_{J\{M\}}$ of 0.2)
  \item H1: variety of obtained sets of support is small (average $D_{J\{M\}}$ less than 0.2)
\end{itemize}
Table~\ref{tab:variety} shows that, with an effect size of 0.79, H0 can be rejected.
\begin{table}
  \centering
  \begin{tabular}{ |c|c|c|c|c|c| }
    \hline
     min & max & mean & stdev & ES & p-value\\[0.5ex]
    \hline
    %sample size = 395
     0.0   & 0.879 & 0.096 & 0.132 & 0.79 & < 0.00001 \\[0.5ex]
    \hline
  \end{tabular}
  \caption{$D_{J\{M\}}$ among all models}
  \label{tab:variety}
\end{table}

%summarized as follows:
%\begin{itemize}
%  \item minimum $D_{J\{M\}}$ among all models: 0.0
%  \item maximum $D_{J\{M\}}$ among all models: 0.879
%  \item average $D_{J\{M\}}$ among all models: 0.096
%  \item standard deviation of $D_{J\{M\}}$ among all models: 0.132
%\end{itemize}

\vspace{6pt}
\noindent\fbox{%
    \parbox{\columnwidth}{%
    An average $D_{J\{M\}}$ of 0.096 shows that variety of support sets computed by \texttt{ReduceSupport} is very small.
    }%
}
 \vspace{6pt}

\subsection{Threats to Validity}

\begin{itemize}
    \item Internal validity: not really necessary, I think.
    \item External validity (how much can the results be generalized)
    \begin{itemize}
        \item currently all models are *very small*;
        \item Many programs were drawn from mutations of a relatively small number of ``seed'' programs;
        \item The models are written in Lustre rather than FOL.  This means that the
            top-level conjunctions are all over equations rather than general
            form;
        \item Others?!?
    \end{itemize}
    \item Construct validity: we are measuring what we think we're measuring: IVC and minimality are reasonably defined.  For discussions of ``completeness'' and ``traceability'' we need to be clear about any claims (probably not in this paper).
\end{itemize}

\mike{Is this necessary?  We are not making many statistical claims.}


