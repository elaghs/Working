\documentclass{article}
\usepackage{xspace}
\usepackage{xcolor}

\begin{document}
%\begin{itemize}

\newcommand{\lbb}{[\![}
\newcommand{\rbb}{]\!]}
\newcommand{\expr}{\phi}
\newcommand{\exprS}{\Phi}
\newcommand{\bool}[0]{\mathit{bool}}
\newcommand{\reach}[0]{\mathit{R}}
\newcommand{\ite}[3]{\mathit{if}\ {#1}\ \mathit{then}\ {#2}\ \mathit{else}\ {#3}}

\newcommand{\mike}[1]{\textcolor{red}{#1}}
\newcommand{\fixed}[1]{\textcolor{purple}{#1}}
\newcommand{\andrew}[1]{\textcolor{green}{#1}}
\newcommand{\ela}[1]{\textcolor{blue}{#1}}
\newcommand{\stateequiv}{\equiv_{s}}
\newcommand{\traceequiv}{\equiv_{\sigma}}

\newcommand{\ivc}{\textit{IVC}\xspace}
\newcommand{\mivc}{\textit{MIVC}}
\newcommand{\aivc}{\textit{AIVC}}
\newcommand{\must}{\textit{MUST}}
\newcommand{\may}{\textit{MAY}}


\newcommand{\bfalg}{\texttt{\small{IVC\_BF}}}
\newcommand{\ucalg}{\texttt{\small{IVC\_UC}}}
\newcommand{\ucbfalg}{\texttt{\small{IVC\_UCBF}}}
\newcommand{\mustalg}{\texttt{\small{IVC\_MUST}}}

\newcommand{\nondetcov}{\text{\sc Nondet-Cov}}
\newcommand{\nondetcovalt}{\text{\sc Nondet-Cov$^{*}$}}
\newcommand{\ivccov}{\text{\sc IVC-Cov}}
\newcommand{\maycov}{\text{\sc May-Cov}}
\newcommand{\mustcov}{\text{\sc Must-Cov}}
\newcommand{\allcov}{\text{\sc Model-Cov}}
\newcommand{\mutcov}{\text{\sc Mutant-Cov}}


\newcommand{\bq}{\textsc{BaseQuery}\xspace}
\newcommand{\iq}{\textsc{IndQuery}\xspace}
\newcommand{\fq}{\textsc{FullQuery}\xspace}

\newcommand{\mink}{\textsc{MinimizeK}\xspace}
\newcommand{\reduceinv}{\textsc{ReduceInvariants}\xspace}
\newcommand{\minivc}{\textsc{MinimizeIvc}\xspace}

\newcommand{\checksat}{\textsc{CheckSat}}
\newcommand{\isadeq}{\textsc{CheckAdq}}
\newcommand{\actlit}{\textsc{ActLit}}
\newcommand{\unsatcore}{\textsc{UnsatCore}\xspace}
\newcommand{\unsat}{\texttt{UNSAT}\xspace}
\newcommand{\sat}{\texttt{SAT}\xspace}

\newcommand{\getivc}{\textsc{GetIVC}}
\newcommand{\getmodel}{\textsc{GetLiteralsFromMaxModel}}
\newcommand{\aivcalg}{\texttt{\small{All\_IVCs}}}
\newcommand{\blockup}{\textsc{BlockUp}}
\newcommand{\blockdown}{\textsc{BlockDown}}
\newcommand{\mis}{\textit{MIS}}
\newcommand{\mcs}{\textit{MCS}}

\newcommand{\unknown}{\textsc{Unknown}}
\newcommand{\adequate}{\textsc{Adequate}}
\newcommand{\inadequate}{\textsc{Inadequate}}

\newcommand{\true}{\textsc{True}}
\newcommand{\false}{\textsc{False}}

\paragraph{R1:} \textit{The main concern I have is that an IVC is not unique, hence any conclusions derived from IVC are not uniquely defined.}

\vspace{0.05in}

\noindent This is a very good point.  We make the point in the introduction: \textit{Depending on the model and property to be analyzed, there is often substantial diversity of possible IVCs used to produce a proof, and there can also be a substantive difference in the size of a {\em minimal} IVC and a {\em minimum} IVC, which is the (not necessarily unique) smallest MIVC.}

Nevertheless, we could state this more clearly, especially as it relates to traceability, so we have added the following to the paragraph on traceability in the introduction:

\textit{One important aspect of traceability that is often left unexamined is that of diversity. often there are multiple traceability matrices that could equally well be represent functional traceability (at least, in terms of proof).  Computing all IVCs exposes this diversity and may lead to a more complete understanding of system behavior.  Conversely, a single IVC may not provide a complete understanding of traceability.}

\ela{Mike, I also add this before the definition of all MIVCs: Let me know if you'd like to change it back:}
Note that, given $(I, T) \vdash P$, $P$ always has at least one \mivc, and it may also have many distinct {\mivc}s corresponding to different proof paths.
For example, for a specification $G(p \cup q)$ and a system that satisfies $p$ and $q$, there are two possible \mivc s. Having the \mivc\ that contains $p$, one might conclude that the specification is vacuous in $q$. However, the set of all \mivc s shows that the property is not vacuous in $q$ or $p$, but it can be satisfied in two unique ways, which can point out other redundancy in the model or fault tolerance. The $\aivc$ relation has been introduced in \cite{Murugesan16:renext} to address such issues.

\ela{and another at the end of the first paragraph of section 4.2:} Note that minimum \mivc\ may not be unique either. And computing all \mivc s reveals all the minimum \mivc s.


\paragraph{R1:} \textit{It is also clear why the authors propose to compute a minimal core, rather than a minimum one, as the computation of a minimum one is, if I am not mistaken, $\Sigma_2$ complete (and it is not unique either - see the example above). Still, it would be helpful to have this discussion in the paper.}
\vspace{0.05in}

\noindent This is correct and insightful.  We have tried to discuss and characterize the relationship between single and all-IVCs (which is considerably more expensive to compute and contains the minimum IVC), but we do not spend much time on minimal vs. minimum IVCs.

We have modified the first paragraph on page 2 to the following:
\textit{IVCs are made up of a set of model elements (a notion that will be made precise in Section~3).  An IVC is {\em minimal} (MIVC) if no element can be removed while preserving provability.  Depending on the model and property to be analyzed, there is often significant diversity of possible IVCs used to produce a proof, and there can also be a substantive difference in the size of a {\em minimal} IVC and a {\em minimum} IVC, which is the (not necessarily unique) smallest MIVC.  Computing a minimum, rather than minimal, IVC is significantly more expensive, because it involves searching through the space of all proofs to find an IVC of (unknown) minimum size.  In this paper, we provide algorithms for finding minimal IVCs and finding {\em all} MIVCs, from which the set of all minimum IVCs can be extracted.}

We do discuss the tradeoffs in efficiency and minimality in the experimental results section (RQ2) and (RQ3).  Since even the initial model checking problem and the computation of single IVCs is undecidable in the general case, we focus primarily on an empirical comparison of single vs. multiple IVCs, as discussed in RQ2 (efficiency) and RQ3 (size).

In rare cases where there are {\em many} IVCs, it might be more efficient to focus the proof search for IVCs based on size, by starting from an initial IVC and looking for strictly smaller solutions by adding a concrete maximum size constraint to the IVC formula.  This would produce at most a linear number of olutions but involves construting a more complex formula.  We have added the following to the future work section:

\textit{
One thing we have not explored is efficient techniques for producing a {\em minimum} as opposed to a {\em minimal} IVC.  Although minimum IVCs can be extracted from the set of all MIVCs, there are circumstances in which computing all MIVCs is impractical because of the number of possible MIVCs for the system.  By adding a size constraint to the MIVC query, it may be possible to search for successively smaller IVCs, leading to a more efficient approach to finding a single minimal IVC in these cases.  However, this makes the formula more complex, and (at least in our experiment) it was rare to have large numbers of MIVCs.}

\ela{Mike, I added this. please remove it if unnecessary: }
\paragraph{R1:} \textit{Note that the same problem also arises when trying to compute coverage based on minimal IVCs. In fact, the example above also demonstrates that a set of states and signals can be considered covered or not covered depending on a particular IVC we consider.}
\vspace{0.05in}
We expanded the explanation after the first coverage metric to emphasize this issue was part of the fact we introduce other metrics that can address such issues:

Note that if an \mivc ~contains all model elements (i.e., the model is {\em completely covered}), then there is only one possible \mivc , so in this case there is no diversity of scores. On the other hand, a set of states and signals can be considered covered or not covered depending on a particular \mivc\ we consider for coverage evaluation. To address this issue, we introduce additional coverage metrics using the notions of \may\ and \must.

\paragraph{R2:} First, a general comment by the authors: the definitional material has been reviewed over several conference papers and has not been previously problematic. Here we provide a point-by-point discussion of the issues raised by Reviewer 2 and hope that these will clarify the discussion.  We think that many of the reviewer's comments are related to a misunderstanding: the sets $S$ and $T$ are sets of {\em conjuncts}, that is, constraints, not sets of {\em states}.  This distinction is very important, as a larger set of conjuncts leads to a smaller set of satisfying states.

We closely examined the introduction and believe that this distinction is already fairly clear in the paper, so we have not made any substantive changes, but have tried to add a few clarifications.

The quotations from reviewer 2 are copied verbatim from the marked up PDF.


\paragraph{R2: Page 5} \textit{$P$ is not necessarily an
over-approximation of R as it may not hold on a reachable state.}
\vspace{0.05in}


\noindent The reviewer is correct, in fact, this is precisely what we want to prove.
Please note that it is not claimed in the paper that $P$ is an overapproximation of the reachable states. As stated by the paragraph immediately after the highlighted text:

\textit{If both formulas hold then $P$ is inductive and holds over the
system. If (1) fails to hold, then $P$ is violated by an initial
state of the system. If (2) fails to hold, then $P$ is too much of an
over-approximation and needs to be refined.}

Note the "if both formulas hold". This is the definition of an
inductively provable argument over transition systems, and is
standard in, for example:

\begin{small}
\begin{verbatim}
@inproceedings{Sheeran:2000:CSP:646186.683237,
 author = {Sheeran, Mary and Singh, Satnam and St{\aa}lmarck, Gunnar},
 title = {Checking Safety Properties Using Induction and a SAT-Solver},
 booktitle = {Proceedings of the Third International
              Conference on Formal Methods in Computer-Aided Design},
 series = {FMCAD '00},
 year = {2000},
 isbn = {3-540-41219-0},
 pages = {108--125},
 numpages = {18},
 url = {http://dl.acm.org/citation.cfm?id=646186.683237},
 acmid = {683237},
 publisher = {Springer-Verlag},
 address = {Berlin, Heidelberg},
}
\end{verbatim}
\end{small}

\paragraph{(R2): Page 6, column 1:} \textit{The set of states satisfying $P$ is exactly the initial states?}

\noindent In this paragraph, we are describing the most general transition system that satisfies the property of interest $P$.  It is used to drive the discussion of the transition system we wish to produce, which is also a generalization of the original transition relation.

\paragraph{(R2): Page 6, column 1:} \textit{How can a binary relation be equal to a unary
one?}
\vspace{0.05in}

\noindent This is a standard notational shorthand: $T(u, u')$ is defined to be the set of constraints on the post-state variables defined in $P$ (that
is, $P(u')$) with no other constraints.

We have added this prose explanation.

\paragraph{(R2) Page 6 column 1:} \textit{$a1\_below$ is a predicate; it is not a binary
relation between states}
\vspace{0.05in}

\noindent By `predicate' do you mean single-state relation? If so, then
any such single-state relation is a two-state relation, but only constraining the primed variables.  To better explain how the translation works for Lustre, in the implementation section (Section 5), we have added a thorough description of the translation for the first model in the paper in Section 5.1.

To facilitate discussion on some of the following comments, here is the
full definition of $T$ for the example. $init$ is a special
variable that is initially true (in $I$) and false thereafter.

\begin{verbatim}
I = { init = true }
T = { (al_below' = (alt1' < 10000)) and
      (a2_below' = (alt2' < 10000)) and
      (a1_above' = (alt1' >= 10100)) and
      (a2_above' = (alt2' >= 10100))  and
      (below' = a1_below' or a2_below) and
      (above_hyst' = a1_above' and a2_above') and
      (doi_on' = if (below' and not inhibit') then true else d1')
      and
      (d1' = if (inhibit' or above_hyst') then false else d2')
      (d2' = if (init) then false else doi_on) and
      (init = false) }
}
\end{verbatim}

We represent each conjunct (and in this case, each conjunct is an equation) by using the name of the variable defined by the equation.

\paragraph{(R2) Page 6 column 1}: \textit{$(I, S)$ is a subtransition system, it is a bit
misleading to call it inductive when there is no guarantee that it
is inductive at all.}
\vspace{0.05in}

\noindent This is not what is being claimed. We state that $(I, S)$ is an
inductive validity core *if-and-only-if* $(I, S) \vdash T$.

\paragraph{(R2) Page 6 column 2}: \textit{inhibit does not appear so how do you evaluate
on\_p}
\vspace{0.05in}

\noindent Inhibit is an input; it is not constrained by the transition
system, so does not have a definition in our set of conjunctions
that define the transition system (as described above).

\newcommand{\implies}[0]{\Rightarrow}
\newcommand{\mkeyword}[1]{\mbox{\texttt{#1}}}

\paragraph{(R2) Page 6, column 2}: \textit{If $S_1$ is a subset of $S_2$ then $S_1 \implies S_2$ should hold}
\vspace{0.05in}

\noindent This is incorrect. $S_1$ and $S_2$ are sets of constraints, and the implication relation is correct as specified in the paper.  We will illustrate with an example. Suppose $S1$ contains claims $\{a, b\}$ connected by conjunction and $S2$ contains $\{a,
b, c\}$.  If we know $S1$ is true, then we do not know whether $S2$ is true: $(a \land
b) \implies (a \land b \land c)$ does not hold in general.  On the other hand: $(a \land b \land c) \implies (a \land b)$ is valid.

Note that if $S_1$ and $S_2$ were sets of states (they are not!), the situation would be reversed, and $S_1 \implies S_2$ would be valid.


\paragraph{(R2) Page 7 column 1}:
\textit{Once again, adding logical formulae restricts the set of states of the transition system satisfying the foirmula so
the reachable ones are potentially less. The relation between syntax and
semantics is countervariant.}
\vspace{0.05in}

\noindent Larger in this context means additional constraints, not more states.  Each added invariant must be justified in terms of proof support: a set of equations from the original problem definition. So, a system with a *superset* of invariants will have a non-strict superset of elements from the original problem definition.



\paragraph{(R2) Page 7, column 2}: Sentence from paper: We assume both CHECKSAT and UNSATCORE are always terminating.  Reviewer comment:
\textit{Without additional hipothesis this is not true
for first order logic.}
\vspace{0.05in}

\noindent SAT and SMT problems usually operate over decidable, quantifier-free fragments of FOL, and this is true of our tools, so decidability is not in question.  Nevertheless, it is possible to construct SMT solvers over undecidable theories like arrays and support for nested quantification. We already have support for undecidability that is discussed in Section 4.2. We add the following footnote:

\textit{SAT problems and SMT problems involving decidable theories (such as the theories used by our tools) will always terminate.  For approaches using undecidable theories, we can relax this restriction using  techniques similar to those discussed in Section~4.2}

%Also, ``hipothesis" is spelled ``hypothesis".

\paragraph{(R2) Page 7, column 2}: \textit{How can you reuse? This is not at all obvious to state as this.}
\vspace{0.05in}

\noindent The reuse is through the standard practice of incremental SMT solving. By performing check-sat under assumptions (as is supported in the SMT lib spec) we can quickly turn on / turn off different activation literals leaving the learned clauses intact. This is
*not* a focus of the paper and would require an explanation of incremental SMT which is outside the scope of this paper.

We have added a footnote: \textit{Efficiency is improved by using solver support for checking SAT-under-assumptions, which allows us to gradually add and remove constraints.}


\paragraph{(R2) Page 8, column 1}: \textit{See previous correction about sat for first
order logic formulae.}
\vspace{0.05in}

\noindent The ``corrections" thus far have been incorrect. Please see our earlier comments.

\paragraph{(R2) Page 8, column 1}: \textit{This proof requires further explanation on the
intuition behind the proposed construction. The intuition is not clear.}
\vspace{0.05in}

\noindent The intuition is as follows: model checking has a certain difficulty depending on the structure of the transition relation.  If propositional, then it is decidable. If infinite state, then it is undecidable in general.  Given an (unknown) model checking problem, we show that determining minimality of an IVC is exactly equivalent to solving this problem.  We do so by constructing an extra clause that is required only if the original model checking problem is false.  Thus, to determine minimality of this system, we must solve the original model checking problem.

%This was not an issue for the reviewers of 3 previous conference papers containing the proof.  We prove exactly what is necessary to show the difficulty of computing IVCs.

\paragraph{(R2) Page 8, column 1}: \textit{In previous paragraphs it was said that SAT for FOL is assumed to be decidable and it is not; and now model-checking is said to be undecidable, which is not. In general, model-checking for undecidable languages is referred to as bounded MC or something like that}
\vspace{0.05in}

\noindent There are many incorrect statements within this comment.  We have claimed to operate over queries checkable by SMT solvers, and these are decidable for the theories of interest in this paper.  E.g., from page 1: \textit{We assume that our queries are checked by an SMT solver}

Model checking is not decidable for infinite state sequential problems.  This is a well-known result.  The claims involving infinite-state model checking demonstrate a lack of understanding of current model checking techniques by the reviewer.  We recommend the book {\em Principles of Model Checking} by Baier and Katoen for an-depth discussion of these topics and the paper {\em Generalized Property-Directed Reachability} by Hoder et. al.  and {\em $IC^{3}$ Modulo Theores via Implicit Predicate Abstraction} by Cimatti et. al. for more specific discussions of inductive techniques for infinite state model checking.


\paragraph{Page 8, column 2}: \textit{What is sequential model-checking in this context?}
\vspace{0.05in}


\noindent Sequential model checking is just model checking. We have corrected this.


\paragraph{(R2) Page 9, column 1}: \textit{Once again, the intuition says that shrinking the transition relation might allow to prove P while a bigger superset would not due to connecting more states of the transition system not satisfying the property.}
\vspace{0.05in}

\noindent The reviewer is considering $S1$ and $S2$ as sets of states; this is incorrect.  $S1$ and $S2$ are sets of constraints connected by conjunction.  So $S1 \subseteq S2$ means that $S1$ has fewer constraints (that is, admits *more states* than $S2$).

\paragraph{(R2) Page 9, , column 2}: \textit{Once again, for first order logic this is not true without additional hypothesis.}
\vspace{0.05in}

\noindent See previous comments on the decidability of SAT and SMT problems.

\paragraph{(R2) Page 10, column 1}: \textit{It is not clear what weighted clauses is.}
\vspace{0.05in}

\noindent We have modified the footnote, which hopefully makes this clearer:
\textit{MaxSAT is defined as the problem of producing a satisfying assignment having a maximal {\em weight}.  Each clause within the problem can be assigned a non-negative weight.}

For a more thorough discussion, the provided references [30, 31] are good sources of information.


\paragraph{(R2) Page 10, column 1 and 2}: \textit{Please recall that “approximately” in this case means that you could be using IVC\_UC.  And CHECKSAT terminates which is not obvious at all.}
\vspace{0.05in}

\noindent We refer the reviewer to the following previous paragraph:

\textit{We also assume there is a function \isadeq\ that checks whether or
not $P$ is provable by a given subset of $T$.  Note that from Theorem 1, finding a minimal IVC is (in general) undecidable if the original checking problem is undecidable.  Thus, for undecidable model checking problems, \isadeq\  can return \unknown ~(after a user-defined timeout) as well as \adequate\ or \inadequate.  For a given set $S$, if our implementation is unable to prove the property, we conservatively assume that the property is falsifiable and set a warning flag $w$ to the user that the results may be approximate.  If $S$ is adequate, a $\mivc$~is computed by \getivc ~and added to set $A$ (lines 10-11). In this case $map$ is constrained by a new
clause in a way described before and shown in line 12.
However, in the
case that $S$ is inadequate or unknown, $map$ is constrained by the corresponding
literals from $T \setminus S$ in line 14.  Finally, if $S$ is unknown, the warning flag $w$ is set to true, as the results may be approximate (lines 15-16).}

\textit{Note that \isadeq ~can be any method that verifies a safety property, such as K-induction, and the \getivc\ function can be any function that returns an (approximately) minimal IVC, such as the \ucalg\ or \ucbfalg\ algorithms. The only requirement is that it follows the definition of an inductive validity core, that is: $S' \leftarrow \getivc (P, S)$ implies that $S' \subseteq S$ and $(I, S') \vdash P$.}

We well understand that the sequential model checking is undecidable.  The termination argument hinges on the fact that this algorithm will timeout yielding a value of UNKNOWN.  In this case, we return a warning to the user that our results may be approximate, as discussed above.

\paragraph{(R2) Page 12, column 2} : MAY and MUST should use italic text.
\vspace{0.05in}

Fixed!

\paragraph{(R2) Page 14, column 2} : Rephrase.
\vspace{0.05in}

Fixed!

\paragraph{(R3)} :
\textit{Specifically, which aspects of the larger traceability picture are supported by IVCs and which are not? The technique provides traceability between properties and models (from which code is automatically generated), but what about all of the other life-cycle aspects of traceability? What about traceability to higher level hazards, architectural decisions, and test cases (if they exist)? How does it all fit together. Further, I assume that there would be some natural language textual requirements which must ultimately be translated into properties and models.       Please describe the context in which IVC would be applied -- especially wrt traceability and certification. It would be really helpful to see how IVC fits into the larger traceability picture for a system that is model driven and proof-based. What other artifacts are likely to be used and how will system-wide traceability be achieved that encompasses a more complete set of artifacts?}
\vspace{0.05in}

We have tried to clarify this considerably.  We have expanded the traceability section considerably, including adding a section (7.1.4) specifically on DO-178C compliance, and more discussion of how this fits into the AGREE tool and architectural reasoning.

\paragraph{(R3)}:
\textit{Ideally, the paper would include a Traceability Information Model (similar to a database schema) showing artifacts, trace links, and the scope of IVC applicability. This would help to address (a).}
\vspace{0.05in}

We have not included a Traceability Information Model; the applicability of the techniques and the reach of IVCs depends directly on which portions of the system one chooses to formalize; discussing the limits of hierarchical verification in safety-critical systems is outside the scope of the paper.

\paragraph{(R3)}:
\textit{(Clarification point) The paper states that ``none of the existing Satisfaction Argument literature discusses that issue that there are often multiple satisfaction arguments between a requirement and its implementation.'' Can you please clarify this? I understand that a specific requirement R1 and its properties may be satisfied using EITHER features A or B in the case of alternate satisfaction arguments. However, if this is the case then shouldn't there also be an additional requirement R2 describing some need for fault tolerance/redundancy -- and if not, then we would make the assumption that the authors claim. So Property P1 might be satisfied either using Features/Components A or B. This would be true; however, there also exists another fault tolerance property P2, which requires redundancy. Can you please address how your system deals with this type of scenario (which seems rather common). Presumably you need to take a more global perspective on determining whether code is superfluous. Actually your approach implies this; but the section on Satisfaction seems to take a more local perspective.}
\vspace{0.05in}

This is a good point (if I understand it correctly).  I believe that the point is: if you have a requirement that can be satisfied in multiple ways, then is it usually the case that you have another requirement such that both paths are required?  For example, you could have a requirement that talks about functional behavior (that can be satisfied in multiple ways) and another that talks about continuity of function even with a single component failure.  If I have understood correctly, then if the properties are specified separately, the system builds IVCs for each property separately.  So, the first property may have two IVCs and the second property one IVC.

Also, we have removed this verbiage from the paper.  The most important aspect is that this traceability information can be generated automatically and accurately; to make such a claim about satisfaction argument literature really should requires additional scoping on which literature is examined: are goal-driven models doing satisfaction?  If so, then perhaps we are being too bold in our claims, because they allow or-decompositions for goal satisfaction.

\paragraph{(R3)}: \textit{When you state that "previous completeness metrics can underapproximate which portions of a program are necessary..." are you referring to non-proof-based techniques e.g., test-case derived metrics?}
\vspace{0.05in}

In this case I was referring to the mutation-based formal coverage metrics, such as those by Chockler.  If you take only the elements that are claimed to be necessary by the metric, you often are no longer able to construct a proof, and in fact the remaining fragment of the model will violate the property of interest.

This is not necessarily a problem with the metrics: they are concerned with completeness of coverage rather than provability, but it sometimes leads to counterintuitive results.  We have also de-emphasized this aspect of the coverage metrics: it is possible that one would like a very strong coverage metric for critical software even if it does not preserve provability.

\paragraph{(R3)}: \textit{experiments focus primarily on the performance (i.e., runtime costs) of computing IVCs, comparing a brute force approach with an UNSAT core-based algorithm. Experiments were conducted on an existing set of benchmarks that included 660 Lustre Models as well as industrial models. The experimentation wrt performance of the model was therefore extensive. Results from the experiments report time penalties in baseline proof time and for computing All-IVCs. However, for this and other experiments, the authors should at least briefly discuss the implications of these results on practice. While I definitely appreciate that the datasets include industrial ones, the experimental analysis could be improved by discussing the implications of performance results wrt to practical use. Please add a couple of sentences to the RQ1-3 sections to discuss practicality of use.}
\vspace{0.05in}

We have added ``takeaway'' summaries in the latest draft of the paper that contains exactly this kind of guidance for each of the research questions.

\paragraph{(R3)}: \textit{As previously mentioned -- experiments focus only on performance, and the application section is devoid of experimentation (with the minor exception that a small experiment on granularity is mentioned in the granularity section). Specific metrics in the applications section could be evaluated, or at least demonstrated. Given that they are proof-based, it would be acceptable to demonstrate them by applying them to at least one system (obviously more would be better). For example, the paper describes a really interesting approach to using mutants to check for requirements completeness and coverage analysis. Given the benchmark datasets, I assume that it would not be difficult to demonstrate the proposed mutant solution as well as other proposed traceability solutions. Applying the approach on at least one of the benchmark datasets would clarify exactly how IVC could support these traceability tasks.}
\vspace{0.05in}

This is a good point.  Roughly speaking, one can generate a linear multiple of mutants based on the number of operators, variables, and constants found within the model.  We have a mutation tool for Lustre programs (designed for testing), which I ran some of the larger scale models.  We can give a rough order of magnitude of performance by simply examining the number of analysis problems.  For the largest model, there are 141742 mutant models possible with our Lustre fault seeder (which is not exhaustive - we have not implemented variable replacement faults).   For the 50 largest models, there are 30k+ possible mutants per model.

We have put in some data related to the number of mutants to provide a rough order of size.  However, it would be difficult for us to run the experiment across the whole data set in the time we have for revisions because it would take a very long time to run.  My concern with only running a subset is that we may skew the data.

It is possible to approximate mutation scores by sampling mutants, but this requires a substantial amount of setup to perform properly: you must think about the distribution of operators and the number of mutants necessary to get a representative sample.  This is outside the scope of the current paper.

\paragraph{(R3)}: \textit{Granularity is critical for any traceability task -- and these results are insightful. Could you report the granularity experiment more formally? In your final paragraph of section 8, which body of work are you referring to when you state that granularity is not discussed in prior work? Is this in the specification work or the traceability literature? In the traceability literature it has been discussed wrt OO design traceability (e.g., class, vs. object, vs. line of code) and in other areas. Please clarify -- I'm assuming you are referring to granularity in the Lustre model.}
\vspace{0.05in}

The work which I was discussing was the literature on coverage as defined on formal models.  We have tried to clarify this section. \mike{MWW: still need to do this!}

\paragraph{(R3)}: \textit{To what extent are your techniques specific to your environment -- or generalizable to other specification languages and models? It appears they would be generalizable to any environment in which liveness and safety properties are modeled.}
\vspace{0.05in}

The algorithms we have constructed work for any inductive model checker, and all current state of the art tools currently use induction (though this may change in the future).  The approach, though demonstrated on safety properties, can be equally well used for liveness properties based on current extensions of inductive tools to liveness (such as $k$-liveness by Cimatti et al)

\paragraph{(R3)}: \textit{The claim that you get traceability for free is both 'true' and 'false' at the same time. True, because you already have the models defined and given those models, you can generate IVCs and achieve traceability, but false because   (a) you may need to decompose the models at levels that you otherwise wouldn't have needed to i.e., granularity discussion, and  (b) because of the cost of building those models.   Please be careful about describing the scope of this claim.}
\vspace{0.05in}

These are all good points.  We have moderated this claim.

\paragraph{(R4) Page 6}: \textit{p. 6 (I am referring to number of pages out of 53, from the submission pdf). You assume that CHECKSAT and UNSATCORE are
always terminating. What happens if these assumptions are violated?}
\vspace{0.05in}

\noindent We have added the following footnote to this sentence to elaborate: \textit{SAT problems and SMT problems involving decidable theories (such as the theories used by our tools) will always terminate.  For approaches using undecidable theories, we can relax this restriction using  techniques similar to those discussed in Section~4.2}.

In our tools, we are working with decidable theories, so decidability is not an issue, but even so, as a practical matter, model checking can time out.  Given UNSAT properties, we have never had an issue with timeouts for UNSATCORE termination; the results that are returned may be approximate, but we believe that the algorithms are terminating.

\paragraph{(R4)}: \textit{I would have liked more info about the implementation of the tool. While you are building on top of JKind, I would have liked to learn about the architecture, implementation effort, etc. In fact, the provided FSE'16 paper describes the implementation in much more detail than in the TSE manuscript.}
\vspace{0.05in}

Fixed!  This was an oversight - we chose the wrong version of the implementation file as the basis of this paper.  Thank you for the catch!

\paragraph{(R4)}: \textit{I would also like more info about the experimental setup, Section 6.1: which benchmarks did you exclude then and why?}
\vspace{0.05in}

\mike{Elaheh: in the text we have the following: ``We selected only benchmark problems consisting of a Lustre model with properties that JKind could prove with a 3-hour timeout.'' Which models remained?  Aren't all of the models in the repo provable with this timeout?  If not, how many were excluded?}

\ela{Mike all the models with valid properties are in there. Only invalid properties are removed. Honestly 3-hour timeout was very generous. Models were proved/disproved in the matter of minute. The longest proof time (close to an hour) was for the QFCS models. That's all. So it is completely true to say that we have all the models with valid properties for which JKind found proof within 3-hours.}

\paragraph{(R4)}: \textit{"Discussion, in my opinion, is the weakest part of the paper. Some of the material presented there is not quite discussion. I know it was presented in earlier papers and the authors want it in the ""big"" one, but the treatment of the material is much lighter than the reset of the paper and having it weakens instead of strengthening the paper, IMHO. Some specific examples: Material in paragraph on p. 15, lines 48-54 is presented too quickly and more details are needed.  But the running example, ASW, does not need to be reintroduced again (lines 55-58).}
\vspace{0.05in}

\mike{Still some TBD...}

We have tried to slim this down, and generally re-balance the paper, including more material from the FSE paper, getting rid of some of the extended discussion on coverage, and include more information on traceabilty and applications.  Also, we have retitled the discussion section to be ``Illustration'' because that is what it is.

\paragraph{(R4)}: \textit{Granularity is a new idea, or at least it is claimed as novel in the contributions, but it is again presented in a way that is too much for a discussion and too little for a technical contribution. Furthermore, section 8.1 is the sole subsection of the paper - a better organization of the section is needed.  The last two paragraphs of section 8.1 are also troublesome - not quite an experiment and not quite a discussion and not quite a contribution.  I would recommend removing granularity from the paper altogether!}
\vspace{0.05in}

We believe that the idea of granularity is conceptually important and should be raised, so as to prevent people from gaining unwarranted confidence in coverage scores.  On the other hand, the ``experimental'' discussion is, as you mention, entirely half-baked.  We have cut this section to focus on why the concept is important.

\mike{Mats, Elaheh - what do you think?}
\ela {Mike I removed granularity from the contributions. I don't know if it should be in a separate section as discussion, but I believe it is necessary to talk about granularity. Maybe as an ending for the coverage section, instead of a new section. But I don't see a problem with having a discussion section over granularity. Sure no contributions, but we should talk about it!}

\paragraph{(R4)}: \textit{Related work is well done, with the exception of the paragraph on p. 18, lines 15-29. It seems to be a separate new thought that should live in the discussion.}
\vspace{0.05in}

Fixed!

\paragraph{(R4)}: \textit{References need to be updated. For some of them, e.g., [59], authors are missing and there are issues with capitalization in the titles. Also, try to avoid "et al" in references.}
\vspace{0.05in}

Fixed!

\paragraph{(R4)}: \textit{Overall, I find the TSE submission to be a bit "loopsided" in terms of the material that the authors chose to include in it. The core FSE material is of outstanding quality but the exclusion of tooling and inclusion of granularity make the composition of the material unusual. The specific focus application on requirements traceability is fine but it does not add to the overall "balance" of the paper.}
\vspace{0.05in}

See earlier comment; we have tried to both extend some of the presentation of the basic algorithms and slim down some of the applications section.



\end{document}
