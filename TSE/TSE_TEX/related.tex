\section{Related work}
\label{sec:related}

%\andrew{This section seems a bit strange since it focuses on MUS more
%  than IVC. Are people doing this for SAT/SMT solving or for model
%  checking?}
Our work builds on top of a substantial foundation using UNSAT cores~\cite{Cimatti2007:UNSAT} to build Minimally Unsatisfiable Subformulas
(MUSes), including \cite{nadel2014accelerated, belov2013core, belov2012towards, ryvchin2011faster, belov2012computing, marques2010minimal}.  Recent algorithms can handle very large problems, but computing MUSes is still a resource-intensive task.  In this work, minimality is usually defined such that given a set of clauses $\mathbb{M}$, removing any member of $\mathbb{M}$ makes it satisfiable \cite{belov2012computing}.

UNSAT cores and MUSes are used for many different activities within
formal verification.  Foremost, they are used for explanations of unsatisfiability (as is performed in Alloy~\cite{Torlak08:cores}).  Gupta et al. \cite{gupta2003iterative} and
McMillan and Amla \cite{mcmillan2003automatic} use 
unsatisfiable cores in proof-based abstraction engines. Their goal is
to shrink the abstraction size by omitting the parts of the design
that are irrelevant to the proof of the property under verification.  Another use of MUSes is to find bi-decomposed Boolean circuits~\cite{6081636}.

The all-IVC computation is based on previous work on finding {\em all} minimal unsatisfiable subsets (all MUSes) for SAT/SMT checking.  While most previous research focuses on the SAT/SMT case specifically, several of the algorithms, such as MARCO~\cite{marco2016fast}, CAMUS~\cite{Liffiton2008}, and DAA~\cite{Bailey:2005:DMU:2158261.2158277} are domain-agnostic, and use an efficient single-MUS extraction algorithm as a black box inside of a loop that explores the space of all possible MUSes.  This means that these approaches can be generalized towards richer verification problems and even domains far outside of logic.  We have chosen MARCO as the basis of all-IVC enumeration algorithm as it has relatively steady rate of producing solutions (this is called an {\em online} algorithm in the MUS literature) and has very good performance in all cases.  For models with small numbers of IVCs, CAMUS outperforms MARCO, but only emits answers at the end of computation (an {\em offline} algorithm), so for models with large or intractable numbers of MUSes, it performs poorly or does not return an answer.

The domain-agnostic nature of these algorithms recently has also been exploited by Bendick et. al, constructing algorithms that can be used for {\em consistency checking}~\cite{DBLP:conf/issta/Bendik17}, {\em safety analysis}~\cite{DBLP:conf/sefm/BendikBBC16}, and different {\em tunable algorithms}~\cite{DBLP:conf/fsttcs/BendikBCB16} that combine aspects of MARCO and CAMUS towards achieving ``best case'' performance over a wide range of problems.  Our work in all-IVCs can be seen as a similar specialization of these algorithms to a different domain.

The IVC idea shares many similarities with approaches for computing minimal invariant sets for inductive proofs (such as is performed for inductive proof certificates~\cite{piskac2016, Ivrii14:invariants}), and in fact our IVC algorithm also needs to find a minimal lemma set and usually outperforms the approach in~\cite{piskac2016}.  However, there is a substantive difference: to find a minimal set of constraints, it is usually necessary to find new proofs involving {\em new lemmas} not used in the original proof, which accounts for the expense of finding an accurate minimal IVC.  

The Cadence Jasper Gold and Synopsys VC Formal commercial hardware verification tools produce~\emph{proof-cores}~\cite{hanna2015formal, jasper_gold, Synopsys_VC_formal}, which we believe to be similar to IVCs/MIVCs.  The proof-core provided by these tools is currently used for internal coverage analysis measurements performed by the tools.  Unfortunately, the techniques provided by the commercial tools are not documented at a sufficient level of detail to perform a theoretical comparison:
no formal description of proof-cores or algorithms are provided by either tool. In addition, no implementations or experimental results are provided, so we are not able to benchmark our techniques against those tools; to do so ourselves would also violate the licenses of the tools.  Finally, to the best of our knowledge, these tools only produce a single proof core per property; there is no support for determining all cores. Our all-IVC work could be used to support this capability in future editions of these tools.

%[SLICING]
If we view Lustre as a programming language, our work can be viewed as a more accurate form of program slicing~\cite{Tip95asurvey}.  We perform {\em backwards slicing} from the formula that defines the property of interest of the model.  The slice produced is smaller and more accurate than a static slice of the formula~\cite{Weiser:1981:slicing}, but guaranteed to be a sound slice for the formula for all program executions, unlike dynamic slicing~\cite{Agrawal:1990:slicing}.  Predicate-based slicing has been used~\cite{Li04:slicing} to try to minimize the size of a dynamic slice.  Our approach may have utility for some concerns of program slicing (such as model understanding) by constructing simple ``requirements'' of a model and using the tool to find the relevant portions of the model.

Another potential use of our work is for ``semantic'' vacuity detection.  A standard definition of vacuity is syntactic and defined as follows~\cite{Kupferman:2006:SCF}: {\em A system K satisfies a formula $\phi$ vacuously iff $K \vdash \phi$ and there is some subformula $\psi$ of $\phi$ such that $\psi$ does not affect $\phi$ in K}.  Vacuity has been extensively studied~\cite{Gurfinkel:2012:RVB,Chockler2008,DBLP:Ben-DavidK13,Kupferman:2006:SCF,Chockler:2007,Beer1997} considering a range of different temporal logics and definitions of ``affect''.  On the other hand, our work can be used to consider a broader definition of vacuity.  Even if all subformulae are required (the property is not syntactically vacuous), it may not require substantial portions of the model, and so may be provable for vacuous reasons.  The problem is exacerbated when the modeling and property language are the same (as in JKind), because whether a subformula is considered part of the model or part of the property, from the perspective of checking tools, can be unclear.

Coverage analysis of properties has also been extensively studied. Certification standards such as DO-178C~\cite{DO178C} require that requirements-derived tests achieve some level of structural coverage (MC/DC, decision, statement) depending on the criticality level of the software, in order to approximate completeness.  If coverage is not achieved, then additional requirements and tests are added until complete coverage is achieved.  Chockler~\cite{chockler_coverage_2003} defined the coverage metrics for formal properties based on mutation coverage.  Later work by Kupferman et al.~\cite{Kupferman:2006:SCF} defines completeness as an extension of vacuity to elements in the model.  We present an alternative approach that uses the proof directly, which we expect to be considerably less expensive to compute.  Recent work by Murugesan~\cite{murugesan2015we} and Schuller~\cite{schuler_assessing_2011} attempts to utilize properties when determining test coverage towards ensuring adequate requirements.


%[DETERMINING VACUITY]
%- Vardi and Kupferman's work


\iffalse
\begin{itemize}
    \item MUS's : checked
    \item Work on Alloy: checked
    \item Work that Teme pointed us to : will be added
    \item Anything else Elaheh has found : \%60 checked
\end{itemize}
\fi

%%  LocalWords:  Subformulas MUSes UNSAT formulae et al Amla Torlak
%%  LocalWords:  IVCs iff subformula subformulae criticality Chockler
%%  LocalWords:  Kupferman Vardi Murugesan Schuller MUS's Teme Elaheh
