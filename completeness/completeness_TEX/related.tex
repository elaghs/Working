\section{Related work}
\label{sec:related}

Different notions of coverage have been well defined in software testing, however, in formal verification, it is very complex to define and compute this notion.
Usually, coverage techniques in the property-based verification try to measure the quality of the specification with regards to the completeness of a set of properties.

Coverage in verification was introduced in \cite{hoskote1999coverage, katz1999have}. Hoskote et al. \cite{hoskote1999coverage} suggested a state-based metric in model checking based on FSM mutations, which are small atomic changes to the design. Then, the method for measuring coverage is to model check a given property for each mutant design.
Later in \cite{chockler_coverage_2003}, Chockler et al. provided corresponding notions of metrics used in simulation-based verification for formal verification. In fact, they improved the same idea of mutation-based coverage where each mutation is generated to check if a specific
design element is necessary for the proof of the property.
 However, the proposed metric is not only very computationally expensive, but also is not able to identify \emph{may} elements of the design. That is, if a proof requires a set of IVCs made of two disjoint sets of \emph{may} and \emph{must}, which both play a part in the validity of the property, their technique can only specify the \emph{must} set. It should be pointed out that most of the mutation-based metrics, including \cite{kupferman_theory_2008, chockler2001practical}, are focused on finite state systems.

A more recent work in \cite{chockler2010coverage} performs coverage analysis through interpolation \cite{mcmillan2003interpolation}. This work is also based on design-dependent mutations \cite{chockler_coverage_2003}, where a design is considered as a net-list with nodes of types \{AND, INVERTER, REGISTER, INPUT\}. Each mutant design changes the type of a single node to INPUT. When property $\phi$ satisfied by the original net-list fails on the mutant design, it is said that a mutant is discovered for $\phi$, which is the same as a \emph{must} element.
Then, the coverage metric for $\phi$ is defined as the fraction of the discovered mutants, based on which the coverage of a set of properties is measured as the fraction of mutants discovered by at least one property.
In order to decrease the cost of computation, the check is performed at several stages; first, all the nodes that do not appear in the resolution proof of a given property are marked as \emph{not-covered}, and the rest of the nodes are marked as \emph{unknown}. Then, for the unknown nodes, the basic mutation check is performed: if a corresponding mutant design violates the property, it will be considered as \emph{covered}. Otherwise, the algorithm tries to drive an inductive invariant to prove that the node is not covered. Finally, an interpolant-based model checking is applied to the nodes that are still unknown.

A Different approach to measure coverage in formal verification is to check each output signal is fully constrained by the specification \cite{das2005formal, claessen2007coverage, grosse2007estimating}, which is not related to the mutation coverage. For example, in In \cite{claessen2007coverage}, authors proposed a design-independent coverage analysis where a forgotten property is identified by an uncovered output signal. This method investigates, given a property list and a specific output signal $s$, if there is a trace with a point in time when a particular $s$ is not constrained by any properties. Another work in \cite{haedicke2012guiding} defines a coverage metric that computes a numerical value to describe how much of the circuit behavior is constrained by a given set of properties.

%This methods investigates, given property $\phi$ and a specific output $s$, if there exist two traces $\sigma_{1}$ and $\sigma_{2}$ that: (1) $\sigma_{1} \vDash \phi$ and $\sigma_{2} \vDash \phi$ (2) $\forall$ signals &s' \neq s, \forall t. \sigma_{1}(t, s') = \sigma_{2}(t, s')& (3) &\exists t. \sigma_{1}(t, s) \neq \sigma_{2}(t, s)&. This method was implemented in SMV model checker \cite{smv}.

Another technique to measure requirement completeness is to employ several surrogate models; for example, Zowghi and Gervasi use refinement to show {\em relative completeness} with respect to a {\em domain} model, which describes the behavior of the real world, irrespective of change induced by software.  In their model, each iteration of refinement of requirements and domain models must be sufficient to prove the requirements of the previous iteration.  However, this idea has two problems: first it provides no notion of absolute completeness, and second, it requires construction of a domain model, which is often difficult and/or expensive to construct. \ela{Mike, add citation please}. In addition, out of the context of formal verification, many authors have theorised and empirically validated conceptual model completeness, which are mostly dependent on a subjective judgement \cite{drechsler2012completeness, firesmith2005your, chang2007finding,katta2013investigating, zowghi2002three}. For instance, Espana et al. \cite{espana2009evaluating} also studied the granularity and completeness of specification by defining some metrics to measure completeness.

%MORE RECENT: \cite{yang2013minimal} \cite{chockler2011incremental} \cite{brillout2009mutation} \cite{bao2014coverage}: not sure if they are super relevant...
%
%MORE:
%\cite{Kupferman:2006:SCF} ? ...

%
%\cite{Whalen07:FMICS} ...


