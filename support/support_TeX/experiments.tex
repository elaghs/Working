\section{Experiments}
\label{sec:exprm}
There are two important solver-based engines in \texttt{JKind} that can be considered as its primary proof-engines: \texttt{PDR} and \texttt{K-induction}. Current version of \texttt{JKind} has a new solver-based engine called \texttt{ReduceSupport}, 
which is an implementation of Algorithm~\ref{alg:set-of-support}. 
One major goal of our experiments was to evaluate the minimality of the support sets computed by \texttt{ReduceSupport}.
To do so, we needed to compare the output of our algorithm with another algorithm
which computes a truly minimal support set, i.e. Algorithm~\ref{alg:naive}. Hence, we extended \texttt{JKind}
with another support computing tool, called \texttt{JSupport}, which is an implementation of Algorithm~\ref{alg:naive}.

As described in~\ref{subsec:jkind}, \texttt{JKind} accepts LUS models for verification.
Such models are made of a set of nodes each of which embodies a set of inputs, outputs, equations, assertions, and properties. For each property, each equation can be considered as an element of an initial set called \textit{\%SUPPORT}. Given initial support set $S$ for property $P$, \texttt{ReduceSupport} finds which of the items in $S$ are necessary to prove $P$. The output of this computation will be a (closely) minimal set of support for $P$. In light of this process, the extended version of \texttt{JKind} we used for the experiments includes:

\begin{itemize}
    \item an algorithm to compute a truly minimal set of support, i.e. \texttt{JSupport}.
    \item given a LUS model, a static crawler which automatically marks all equations of a node in the initial support set of a property.
    \item some trackers that measure the verification time with/ without support computation.
    \item some minor changes in the XML writers.
\end{itemize}

Our Experiments were performed using the set of benchmarks drawn from the set of single-property benchmarks from \cite{benchmarks}. The benchmarks contain 700 LUS models with properties that are either valid, invalid, or unprovable. First thing we needed to do was to polish the benchmarks and exclude models with invalid properties because a set of support for an invalid property makes no sense. So, finally, experiments were performed on 405 LUS models in the polished benchmarks on an Intel(R) Core(TM) i5-2430M, 2.40GHz, 4GB memory machine.

To evaluate the dependency of our algorithm on different solvers and engines, for each LUS model, \texttt{JKind} was run with 13 different configurations and a timeout of 700 seconds. We chose four SMT solvers: \texttt{Z3}, \texttt{Yices}, \texttt{MathSAT}, and \texttt{SMTInterpol} as well as three engine configurations: \texttt{PDR},
\texttt{K-induction}, and both of them. In other words, the \texttt{ReduceSupport} engine was run with 12 combinations of those settings. And, one additional configuration is where \texttt{JSupport} computes a minimal support set. Therefore, experiments are based on these $405 \times 13 = 5265$ \texttt{JKind} runs. It is worth mentioning that we were to add randomness to the solvers and extend these 13 configurations. However, after performing some initial experiments and analyses, it turns out that random-seed in solvers does not affect the output of our algorithm; hence, it was not considered in the configurations.\footnote{The benchmarks and all the raw experimental results are available on \cite{expr}. The directory also contains mined data obtained from the raw results.}

\subsection{Results}
With a 700 seconds timeout, 10 models had unknown property. In other models with valid properties, for 112 models, all solvers with \texttt{K-induction} setting failed to prove the properties. That is to say, in $(112 \times 4) + (10 \times 13) = 578$ runs, \texttt{JKind} was unable to to prove the property, so the set of support in those runs remained unknown as well. As far as \texttt{JSupport} concerned, support computation in 18 models timed out (with 700 second limitation). 

\subsection{Evaluation}
\label{subsec:eval}
\input{eval}
