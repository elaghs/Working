\section{Introduction}
\label{sec:intro}
%Do we have adequate requirements for our software?  This question has bedeviled the software development community perhaps as long as software has been written.  It has been a subject of active research since at least 1979, when it was the focus of the original TRW technical report on ``requirements engineering''~\cite{}.  A widely accepted view from Zowghi et. al~\cite{Zowghi} is that requirements adequacy can be defined in terms of the `three Cs'': Consistency, Completeness, and Correctness, where consistency is concerned with determining whether any requirements conflict with one another, completeness examines whether you have enough requirements, and correctness examines each requirement to determine whether it correctly characterizes a system behavior.  The focus of this paper is on {\em completeness}, which has been considered the most difficult adequacy criteria to define and analyze (Davis~\cite{}).

%\mike{TRW report: Software Requirements Engineering Methodology (Development) Alford, M. W. and Lawson,J. T. TRW Defense and Space Systems Group. 1979.}

%Boehm~\cite{}\ela{fill in} states that a complete set of requirements has three characteristics: (1) No information is left unstated or ``to be determined'', (2) The information does not contain any undefined objects or entities, (3) No information is missing from this document.  The first two characteristics are typically referred to as internal completeness.  {\em External} completeness (characteristic 3) of the document ensures that all of the information required for problem definition is found within the specification.  {\em Assessing} external completeness in a precise and formal way is difficult, because there is rarely an external reference that can be used to determine whether all relevant requirements have been defined.

%One approach for measuring completeness (\cite{Heimdahl_ConstencyAndCompleteness, Others}) examines whether the requirements form a {\em complete function} over inputs (or input traces, for embedded systems); that is, for a given input (trace), the requirements define a deterministic output (trace).  However, definition allows little freedom in implementations, and is usually considered too strict.  Instead, usually the {\em relative completeness} of requirements with respect to some other artifact.  The other artifact is often some implementation of the requirements.  This idea underlies certification standards such as DO178B/C~\cite{}, which require that requirements-based tests are sufficient to achieve structural coverage of the source code to a certain level of rigor.  If the tests are insufficient, then additional tests (and often, additional requirements) must be written.

%which could be source code, object code, an abstract ``model'' of the implementation, or even lower-level requirements.
%More recent work by Zeller~\cite{schuler_assessing_2011} and Murugesan~\cite{murugesan2015we} have attempted to adapt these measure towards automated test generation by examining coverage of {\em assertions} in the code.

%The testing approach requires that an implementation and test vectors to exist prior to performing this analysis, which has several drawbacks.  If the implementation is only available late in the development process, then incompleteness in requirements is not exposed until the implementation is constructed, potentially leading to substantial rework.  In addition, for large systems, large numbers of tests must be constructed, and these must be updated as requirements change.  Finally, the test metrics tend to substantially overapproximate which portions of a program are necessary to fulfill a requirement~\cite{Whalen13:OMCDC, chelenski1994oapplicability}.

For critical systems, it has been argued that formal methods
%, involving formalized requirements and proofs of implementation correctness,
should be applied to gain higher assurance than is possible with testing~\cite{Miller10:CACM,Rushby09:SEFM,Hardin09:Security}.  For these formal approaches, testing may still be performed, but the verification effort is primarily focused on performing proofs.  Unfortunately, proof-based approaches tend not to answer the question as to whether implementations have {\em additional functionality} that is not covered by requirements.  Testing, despite its faults, can measure {\em structural coverage} to find untested functionality and can find some errors by {\em serendipity}, in which problems not directly related to the requirement under test are exposed.  Therefore, in formal verification approaches, it is even more important that requirements be adequate.

Relatively recently, techniques have been devised for analyzing completeness of requirements against formal implementation models specified as transition systems or Kripke structures \cite{chockler2001practical,das2005formal, claessen2007coverage, grosse2007estimating}.  These models are agnostic to the abstraction level of the implementation: implementations can be lower-level requirements, software architectures, or concrete implementations of system behavior.  The mechanism used is based on {\em mutation} and {\em proof}: is it possible to prove that the requirements still hold of the system after mutating the model in some way?  If so, then the requirements are incomplete with respect to the mutated part of the model.

Unfortunately, previous approaches to computing coverage metrics can {\em underapproximate} which portions of a program are necessary to fulfill the requirements.  That is, if we construct a model consisting of only the model elements marked as covered by the analysis, it is often no longer possible to prove the requirement.  Thus, the feedback provided to the developer may be somewhat misleading.
In addition, the mutation-based analyses tend to be computationally very expensive because there are many possible mutant models to verify.  For example, for model checkers, state of the art techniques have runtimes of (in the best case) several times more than is required for proof~\cite{chockler2010coverage}.

What we would like to have is a graduated set of metrics for checking the adequacy of requirements against an implementation model that:
\begin{itemize}
    \item can be applied early and throughout a development cycle on different implementation artifacts,
    \item are {\em proof preserving}: the portion of the implementation that is identified as necessary demonstrates the
        fulfillment of the requirement but does not contain irrelevant information, and
    \item are efficient to compute.
\end{itemize}

\noindent Towards this end, we propose several notions of requirements adequacy based on {\em minimal proofs of requirements}.  We measure the adequacy of a set of requirements by examining an (approximately) minimal set of model elements necessary to construct a proof of all the requirements.  Like earlier approaches proposed for formal verification, this idea is implementation agnostic so it can be applied early in the development cycle against abstract implementation models.  The approach is implemented using {\em Inductive Validity Cores} (IVCs)~\cite{Ghass16} for transition systems.

We have built support for generating these metrics into a branch of the JKind model checker~\cite{jkind}.  Using a large benchmark suite, we demonstrate that one of the proof-based metrics is considerably more computationally tractable than previous approaches based on mutation, averaging \~20\% overhead over model-checking alone, rather than (for our benchmark problems) the up to~2300\% overhead required for the state of the art of the mutation-based metrics.

Thus, the contributions of this work are:
\begin{enumerate}
\item A family of coverage metrics for formal verification based on \emph{minimal} Inductive Validity Cores (MIVCs).  Most of the proposed metrics are {\em proof preserving},
\item A discussion of the relationship between proof-based metrics and mutation-based metrics, including a proof of equivalence between non-deterministic mutation coverage and one of the proof-based metrics (\mustcov).
\item An implementation that efficiently computes property coverage over symbolic transition systems,
\item An experiment that compares our technique against a state of the art mutation-based notion of completeness.
\end{enumerate}
\vspace{-0.01in}
\noindent Our goal is to provide a range of metrics for describing requirements adequacy using formal methods that are acceptable to certification authorities.  Toward this end, the metrics are now being used by our industrial partner\footnote{The partner is not identified to preserve author anonymity; if accepted, we will of course include the organization.} in a pilot project towards building safety-critical avionics software.  We discuss how the metrics are being used and how they might satisfy certification guidance.


%\mike{something here about certification?}

The rest of the paper is organized as follows.  In Section~\ref{sec:example}, we present a running example to illustrate the concepts formalized throughout the paper.  In Section~\ref{sec:background}, we provide the formal preliminaries for the approach and some background on mutation-based coverage notions.  Section~\ref{sec:method} presents our method for computing relative completeness.
In Section~\ref{sec:illust}, we illustrate how the new approach can be employed in the assessment of requirements completeness.
Section~\ref{sec:impl} provides detail on algorithms and implementation. Section~\ref{sec:experiments} describes an experiment to evaluate our algorithm comparing it with recent work by Chockler and Kroening~\cite{chockler2010coverage}.
Section~\ref{sec:discussion} discusses limitations of adequacy metrics.  In Section~\ref{sec:related}, we cover related work.  Finally, Section~\ref{sec:conclusion} mentions some conclusions and future work.

